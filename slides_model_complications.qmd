---
title: "Model Complications"
execute:
  error: true
---

```{r}
#| label: setup
#| include: false
library(here)
source(here("scripts","check_packages.R"))
source(here("scripts","set_theme.R"))
source(here("scripts","load_example_data.R"))
source(here("scripts", "preamble.R"))
load(here("data", "gss_relig.RData"))
load(here("data", "births_nyc.RData"))
set.seed(666)
```

# The Linear Model Revisited {.center background-color="black" background-image="images/jeremy-bishop-FzrlPh20l7Q-unsplash.jpg"}

## Review of linear model {.smaller}

$$\hat{y}_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}$$

- $\hat{y}_i$ is the predicted value of the outcome/dependent variable which must be quantitative.
- The $x$ values are different independent/explanatory variables which may be quantitative or categorical (by using dummy coding).
- The $\beta$ values are the slopes/effects/coefficients that, *holding all other independent variables constant*, tell us the:
    - The predicted change in $y$ for a one unit increase in $x$ if $x$ is quantitative
    - The mean difference in $y$ between the indicated category and the reference category if $x$ is categorical.
    
. . . 

### Interpret these values

```{r}
#| label: lm-example
coef(lm(nominations~nsports+I(parent_income-45)+gender, data=popularity))
```

## The residual term {.smaller}

$$\epsilon_i=y_i-\hat{y}_i$$

The residual/error term $\epsilon_i$ gives us the difference between the actual value of the outcome variable for a given observation and the value predicted by the model.

. . . 

Lets use algebra to rewrite this equation with $y_i$ on the left-hand side:

$$y_i=\hat{y}_i+\epsilon_i$$

. . . 

If we plug in our linear model formula for $\hat{y}_i$, we can get the full model formula:

$$y_i=\underbrace{\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}}_{\hat{y}_i}+\epsilon_i$$

## Linear model as a partition of the variance in $y$ {.smaller}

$$y_i = \underbrace{\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}}_\text{structural} +
    \underbrace{\epsilon_i}_\text{stochastic}$$
    
- The structural part is the predicted value from our model which is typically a linear function of the independent variables.
- The stochastic component is the leftover residual or error component, that is not accounted for by the model.

. . . 

Depending on disciplinary norms, there are different conceptual ways to view this basic relationship:

- **Description:** observed = summary + residual
- **Prediction:** observed = predicted + error
- **Causation:** observed = true process + disturbance

## Calculating marginal effects {.smaller}

The **marginal effect** of $x$ on $y$ is simply the predicted change in $y$ for a one unit increase in $x$ from its current value, holding all else constant.

. . . 

In a basic linear model, the marginal effect is just given by the slope itself. 

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\epsilon_i$$

- $\beta_1$ is the marginal effect of $x_1$
- $\beta_2$ is the marginal effect of $x_2$

. . . 

Technically, the marginal effect is the derivative of $y$ with respect to a given $x$. This gives us the **tangent line** for the curve at any value of $x$.

$$\frac{\partial y}{\partial x_1}=\beta_1$$
$$\frac{\partial y}{\partial x_2}=\beta_2$$

`r emo::ji("scream")` I am not expecting you to know the calculus behind this, but it may help some people.

## Marginal effects with interaction terms {.smaller}

:::: {.columns}

::: {.column .fragment}

Marginal effects can get more complicated when we move to more complex model. Consider this model with an interaction term:

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3(x_{i1})(x_{i2})+\epsilon_i$$

::: {.fragment}

The marginal effects are now given by:

$$\frac{\partial y}{\partial x_1}=\beta_1+\beta_3x_{i2}$$
$$\frac{\partial y}{\partial x_2}=\beta_2+\beta_3x_{i1}$$

This is a very math-y way of saying: **the main effects depend on the effect of the other variable**.

:::

:::

::: {.column .fragment}

```{r marginal-fx}
model <- lm(nominations~nsports*gender, data=popularity)
as.data.frame(coef(model))
```

::: {.fragment}

The marginal effect for number of sports played is:

$$0.49-0.09(male_i)$$

The marginal effect for gender is:

$$-0.54-0.09(nsports_i)$$

:::

:::

::::

## Marginal effects plot {.smaller}

:::: {.columns}

::: {.column}

```{r}
#| label: fig-marginal-fx-plot-code
#| eval: false
nsports <- 0:6
gender_diff <- -0.54-0.09*nsports
ggplot(tibble(nsports, gender_diff),
       aes(x=nsports, y=gender_diff))+
  geom_line()+
  labs(x="number of sports played",
       y="predicted popularity difference between boys and girls")
```

:::

::: {.column}

```{r}
#| label: fig-marginal-fx-plot-exec
#| fig-cap: Marginal effect of gender difference on popularity by sports played
#| echo: false
nsports <- 0:6
gender_diff <- -0.54-0.09*nsports
ggplot(data.frame(nsports, gender_diff),
       aes(x=nsports, y=gender_diff))+
  geom_line()+
  labs(x="number of sports played",
       y="predicted popularity difference between boys and girls")
```

:::

::::

## Two key assumptions of linear models {.smaller}

:::: {.columns}

::: {.column .fragment}

### Linearity
```{r linear-violation, echo=FALSE, fig.height=4}
ggplot(subset(gapminder, year==2007), aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  geom_text_repel(data=subset(gapminder, 
                              year==2007 & gdpPercap>5000 & lifeExp<60), 
                  aes(label=country), size=2)+
  labs(x="GDP per capita", y="Life expectancy at birth", subtitle = "2007 data from Gapminder")+
  scale_x_continuous(labels=scales::dollar)
```

* If you fit a model with the wrong functional form, it is considered a *specification error*.
* We can correct this through a variety of more advanced model specifications.


:::

::: {.column .fragment}

### Error terms are iid
```{r heteroskedasticity, echo=FALSE, fig.height=4}
model <- lm(box_office~metascore, data=movies)
ggplot(augment(model), aes(x=.fitted, y=.resid))+
  geom_jitter(alpha=0.2)+
  geom_hline(yintercept = 0, linetype=2)+
  labs(x="predicted values of box office returns by tomato rating", y="model residuals")
```
* iid = **independent and identically distributed** which is typically violated either by **heteroscedasticity** or **autocorrelation**. 
* The consequence of violating the i.i.d. assumption is usually incorrect standard errors. 
:::

::::

## How are linear model parameters estimated? {.smaller}

`r emo::ji("warning")` Heavy math ahead!

- We have simple formulas for the slope and intercept for a bivariat model. 
- With multiple independent variables, a simple formula will not suffice. To estimate model parameters with multiple independent variables we need to use some matrix algebra. 

. . . 

### The matrix algebra approach to linear models {.smaller}

We can use matrix algebra to represent our linear regression model equation using one-dimensional **vectors** and two-dimensional **matrices**. 

$$\mathbf{y}=\mathbf{X\beta+\epsilon}$$

- $\mathbf{y}$ is a vector of known values of the independent variable of length $n$.
- $\mathbf{X}$ is a matrix of known values of the independent variables of dimensions $n$ by $p+1$.
- $\mathbf{\beta}$ is a vector of to-be-estimated values of intercepts and slopes of length $p+1$.
- $\mathbf{\epsilon}$ is a vector of residuals of length $n$ that will be equal to $\mathbf{y-X\beta}$.

## `r emo::ji("warning")` Linear model in matrix form {.smaller}

::: {.columns}

::: {.column}

### Known Quantities

$$\mathbf{y}=\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{pmatrix}$$



$$\mathbf{X}=\begin{pmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1p}\\
1 & x_{21} & x_{22} & \ldots & x_{2p}\\
\vdots & \vdots & \ldots & \vdots\\
1 & x_{n1} & x_{n2} & \ldots & x_{np}\\
\end{pmatrix}$$

:::

::: {.column}

### Need to Estimate

$$\mathbf{\beta}=\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{p}\\
\end{pmatrix}$$

$$\mathbf{\epsilon}=\begin{pmatrix}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{n}\\
\end{pmatrix}$$

:::

::::

## `r emo::ji("warning")` Linear model in matrix form 

Putting it all together gives us this beast `r emo::ji("japanese_ogre")`:
$$\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{pmatrix}=\begin{pmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1p}\\
1 & x_{21} & x_{22} & \ldots & x_{2p}\\
\vdots & \vdots & \ldots & \vdots\\
1 & x_{n1} & x_{n2} & \ldots & x_{np}\\
\end{pmatrix}\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{p}\\
\end{pmatrix}+\begin{pmatrix}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{n}\\
\end{pmatrix}$$

## `r emo::ji("warning")` Minimize sum of squared residuals {.smaller}

Our goal is to choose a $\mathbf{\beta}$ vector that minimizes the sum of squared residuals, $SSR$, which is just given by the $\epsilon$ vector squared and summed up. We can rewrite the matrix algebra formula to isolate $e$ on one side:

$$\begin{align*}
\mathbf{y}&=&\mathbf{X\beta+\epsilon}\\
\epsilon&=&\mathbf{y}-\mathbf{X\beta}
\end{align*}$$

. . . 

In matrix form, $SSR$ can be represented as a function of $\mathbf{\beta}$, like so:

$$\begin{align*}
SSR(\beta)&=&(\mathbf{y}-\mathbf{X\beta})'(\mathbf{y}-\mathbf{X\beta})\\
&=&\mathbf{y}'\mathbf{y}-2\mathbf{y}'\mathbf{X\beta}+\mathbf{\beta}'\mathbf{X'X\beta}
\end{align*}$$

## `r emo::ji("warning")` Minimize sum of squared residuals {.smaller}

$$\begin{align*}
SSR(\beta)&=&(\mathbf{y}-\mathbf{X\beta})'(\mathbf{y}-\mathbf{X\beta})\\
&=&\mathbf{y}'\mathbf{y}-2\mathbf{y}'\mathbf{X\beta}+\mathbf{\beta}'\mathbf{X'X\beta}
\end{align*}$$

We want to choose a $\mathbf{\beta}$ to plug into this function that provides the smallest possible value (the minimum). 

. . . 

It turns out that we can get this value by using calculus to get the derivative with respect to $\mathbf{\beta}$ and solving for zero:

$$0=-2\mathbf{X'y}+2\mathbf{X'X\beta}$$

. . . 

Applying some matrix algebra will give us the estimator of $\mathbf{\beta}$:

$$\mathbf{\beta}=(\mathbf{X'X})^{-1}\mathbf{X'y}$$

## Estimating linear models manually

```{r matrixreg}
#set up the design matrix
X <- as.matrix(cbind(rep(1, nrow(movies)), movies[,c("runtime","box_office")]))
#the outcome variable vector
y <- movies$metascore
#crossprod(X) will do matrix multiplication, solve will invert
beta <- solve(crossprod(X))%*%crossprod(X,y)
beta
```

. . . 

```{r}
#how does it compare to lm?
model <- lm(metascore~runtime+box_office, data=movies)
coef(model)
```


## `r emo::ji("warning")` Estimating standard errors {.smaller}

If we treat $\sigma^2$ as the variance of the error term $\epsilon$, then we can also use matrix algebra to calculate the **covariance matrix**:

$$\sigma^{2}(\mathbf{X'X})^{-1}$$

The values of this matrix give us information about the correlation between different independent variables. Most importantly, the square root of the diagonal values of this matrix are the standard errors for the estimated values of $\beta$. 

In practice, we don't have $\sigma^2$, but we can estimate from the fitted values of $y$ by:

$$s^2=\frac{\sum(y_i-\hat{y}_i)^2}{n-p-1}$$
We can then use these estimated standard errors to calculate t-statistics and p-values, confidence intervals, and so on. 

## Calculating SEs manually {.smaller}

```{r matrixse}
y.hat <- X%*%beta
df <- length(y)-ncol(X)
s.sq <- sum((y-y.hat)^2)/df
covar.matrix <- s.sq*solve(crossprod(X))
se <- sqrt(diag(covar.matrix))
t.stat <- beta/se
p.value <- 2*pt(-1*abs(t.stat), df)
data.frame(beta, se, t.stat, p.value)
summary(model)$coef
```


# Modeling Non-Linearity {.center background-color="black" background-image="images/fabian-quintero-nq02weVF_mk-unsplash.jpg"}

## Linear models fit straight lines

```{r}
#| label: fig-life-exp-non-linear
#| fig-cap: The relationship between life expectancy and GDP per capita is clearly not a straight line!
#| echo: false
#| fig-width: 24
#| fig-height: 14

gapminder |>
  filter(year == 2007) |>
  ggplot(aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  geom_text_repel(data=subset(gapminder, year==2007 & gdpPercap>5000 & lifeExp<60), 
                  aes(label=country), size=2)+
  labs(x="GDP per capita", y="Life expectancy at birth", subtitle = "2007 data from Gapminder")+
  scale_x_continuous(labels=scales::dollar)
```

::: {.notes}

- If you try to model a relationship using a straight line when that relationship is more complex (i.e. "bendy") then your model will not fit well and will not represent the relationship accurately. 
- The technical way of saying this is that the functional form of the relationship is mis-specified in the model. 
- The most common cases of non-linearity are **diminishing returns** and **exponential** relationships, although more complex forms of non-linearity are also possible. 
- The example to the left shows a clear diminishing returns between life expectancy and GDP per capita.

:::

## Non-linearity can be hard to detect

```{r}
#| label: fig-non-linear-movies
#| fig-cap: Is this relationship non-linear?
#| echo: false
#| fig-width: 24
#| fig-height: 14

ggplot(movies, aes(x=rating_imdb, y=box_office))+
  geom_jitter(alpha=0.2)+
  scale_y_continuous(labels = scales::dollar)+
  labs(x="IMDB rating", y="Box Office Returns (millions USD)")
```

## Two techniques for detecting non-linearity

::: {.columns}

::: {.column .fragment}

### Non-linear smoothing

```{r}
#| echo: false

gapminder |>
  filter(year == 2007) |>
  ggplot(aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(se=FALSE)+
  geom_text_repel(data=subset(gapminder, year==2007 & gdpPercap>5000 & lifeExp<60), 
                  aes(label=country), size=2)+
  labs(x="GDP per capita", y="Life expectancy at birth", subtitle = "2007 data from Gapminder")+
  scale_x_continuous(labels=scales::dollar)
```

:::

::: {.column .fragment}

### Diagnostic residual plot

```{r}
#| echo: false

model <- lm(lifeExp~gdpPercap, data=subset(gapminder, year==2007))
ggplot(augment(model), aes(x=.fitted, y=.resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_smooth(se=FALSE)+
  labs(x="fitted values of life expectancy", y="model residuals")+
  theme_bw()
```

:::

::::

## Non-linear smoothing

- A smoothing function uses the values of the $y$ for the closest neighbors for a given observation of $x$ to calculate a smoothed value of $y$ that will reduce extreme values.
- Smoothing functions vary by 
     - What function is used to calculate the smoothed values (e.g. mean, median)
     - The span of how many neighbors are considered. Larger spans will lead to smoother lines.

## Median smoothing box office returns for *Rush Hour 3*

:::: {.columns}

::: {.column width = "30%"}

![Rush Hour 3 Poster](images/rushhour3.jpg)

:::

::: {.column width = "70%"}

```{r}
#| label: tbl-smooth-rush-hour-3
#| tbl-cap: Calculating smoothed box office returns for Rush Hour 3 with two neighbors either way.
#| echo: false

movies <- movies |> arrange(rating_imdb)
i <- which(movies$title=="Rush Hour 3")

temp <- movies |>
  slice((i-2):(i+2)) |>
  select(title, rating_imdb, box_office)
temp$box_office_smooth <- c(NA, NA, median(temp$box_office), NA, NA)
temp |>
  gt() |>
  sub_missing(missing_text = "-") |>
  fmt_currency(starts_with("box_office"), decimals = 1) |>
  cols_label(title = "Title",
             rating_imdb = "IMDB Rating",
             box_office = "Box Office (millions)",
             box_office_smooth = "Smoothed Box Office (median)")

```

:::

::::

## Applying a median smoother

```{r}
#| label: fig-median-smoothing
#| fig-cap: Different median smoothers over scatterplot movie box office returns by IMDB rating
#| echo: false
#| fig-width: 24
#| fig-height: 14

smoothed <- movies |> 
  select(rating_imdb, box_office) |>
  mutate(box_office_smooth5 = runmed(box_office, 5),
         box_office_smooth501 = runmed(box_office, 501)) |>
  select(-box_office) |>
  pivot_longer(cols = starts_with("box_office_"), 
               names_prefix = "box_office_", 
               names_to = "smoothing", 
               values_to = "box_office") |>
  mutate(smoothing = factor(smoothing, 
                            levels = c("smooth5", "smooth501"),
                            labels = c("2 neighbors each way", 
                                       "250 neighbors each way")))

ggplot(movies, aes(x = rating_imdb, y = box_office))+
  geom_point(alpha = 0.2)+
  geom_point(data=subset(movies, title=="Rush Hour 3"), color = "red")+
  geom_line(data = smoothed, aes(color = smoothing), linewidth = 1.5)+
  scale_color_manual(values = wes_palette("Moonrise2"))+
  geom_label_repel(data=subset(movies, title=="Rush Hour 3"),
                   aes(label=title), min.segment.length = 0)+
  labs(x="IMDB rating", y="Box Office Returns (millions)")
```

## The LOESS smoother {.smaller}

:::: {.columns}

::: {.column}

### LOcally Estimated Scatterplot Smoothing

- Each smoothed value is determined from a model that includes:
  - polynomial terms (more on that later)
  - weighting of observations by distance from focal point
- LOESS does very nice smoothing but is computationally expensive
- Because of the weighting, LOESS can and does take a very large span of data for each focal point

:::

::: {.column}

```{r}
#| label: fig-loess
#| fig-cap: Use LOESS smoothing for nice smooth lines
#| code-line-numbers: "3"
ggplot(movies, aes(x=rating_imdb, y=box_office))+
  geom_jitter(alpha=0.2)+
  geom_smooth(method="loess", se=FALSE)+
  labs(x="IMDB Rating", 
       y="Box Office Returns (millions USD)")
```

:::

::::

## Adjusting span {{< fa user-ninja >}} {.smaller}

:::: {.columns}

::: {.column}

```{r}
#| label: adjust-span-code
#| eval: false
#| code-line-numbers: "4-9"

ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="loess", span=1, 
              se=FALSE, color="green")+
  geom_smooth(method="loess", span=0.75, 
              se=FALSE, color="red")+
  geom_smooth(method="loess", span=0.25, 
              se=FALSE, color="blue")+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")+
  scale_x_continuous(labels=scales::dollar)
```

The default `span` is 0.75 which is 75% of observations.

:::

::: {.column}

```{r}
#| label: adjust-span-exec
#| echo: false
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="loess", span=1, 
              se=FALSE, color="green")+
  geom_smooth(method="loess", span=0.75, 
              se=FALSE, color="red")+
  geom_smooth(method="loess", span=0.25, 
              se=FALSE, color="blue")+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")+
  scale_x_continuous(labels=scales::dollar)
```

:::

::::

## `r emo::ji("warning")` LOESS with large `n`  will `r emo::ji("death")` your `r emo::ji("computer")` {.smaller}

:::: {.columns}

::: {.column}

### Use a GAM instead {.smaller}

Generalized Additive Models (GAM) are another way to create non-linear smoothing that is less computational intensive that LOESS but with similar results.

```{r}
#| label: gam-example-code
#| eval: false
#| code-line-numbers: "6-7"
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="loess",
              se=FALSE, color="blue")+
  geom_smooth(method="gam",
              se=FALSE, color="red")+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")+
  scale_x_continuous(labels=scales::dollar)
```

:::

::: {.column}

```{r}
#| label: gam-example-exec
#| echo: false
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="loess",
              se=FALSE, color="blue")+
  geom_smooth(method="gam",
              se=FALSE, color="red")+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")+
  scale_x_continuous(labels=scales::dollar)
```

:::

::::

## Or let `geom_smooth` figure out best smoother {.smaller}

:::: {.columns}


::: {.column}

```{r}
#| label: geom-smooth-code
#| eval: false
ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width=1)+
  geom_smooth(method="lm", se=FALSE, 
              color="blue", linewidth = 1.5)+
  geom_smooth(se=FALSE, color="red", linewidth = 1.5)+
  scale_y_continuous(labels = scales::dollar)+
  labs(x="age", y="hourly wages")
```

- For datasets over 1000 observations, `geom_smooth` will use GAM and otherwise defaults to LOESS. 
- Don't forget you can also specify a linear fit with `method="lm"`.

:::

::: {.column}

```{r}
#| label: geom-smooth-exec
#| echo: false
ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width=1)+
  geom_smooth(method="lm", se=FALSE, 
              color="blue")+
  geom_smooth(se=FALSE, color="red")+
  scale_y_continuous(labels = scales::dollar)+
  labs(x="age", y="hourly wages")
```

:::

::::

## Residual plots

:::: {.columns}

::: {.column}

- A scatterplot of the residuals vs. the fitted values from a model can also be useful for detecting non-linearity.
- If the relationship is linear, then we should expect to see no sign of a relationship in this plot. Drawing a smoothed line can be useful for this diagnosis.
- Residual plots can also help to detect **heteroskedasticity** which we will talk about later.

:::

::: {.column}

```{r}
#| label: fig-resid-plot-le
#| fig-cap: Residual plots can be used for multiple diagnostic purposes
#| echo: false

model <- lm(lifeExp~gdpPercap, data=subset(gapminder, year==2007))
ggplot(augment(model), aes(x=.fitted, y=.resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_smooth(se=FALSE)+
  labs(x="fitted values of life expectancy", y="model residuals")
```

:::

::::

## Using `broom` and `augment` to get model data

```{r}
#| label: augment-broom

library(broom)
model <- lm(lifeExp~gdpPercap, data=subset(gapminder, year==2007))
augment(model)
```

## Creating a residual plot

:::: {.columns}

::: {.column}

```{r}
#| eval: false

ggplot(augment(model), 
       aes(x=.fitted, y=.resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_smooth(se=FALSE)+
  labs(x="fitted values of life expectancy", 
       y="model residuals")
```

:::

:::{.column}

```{r}
#| echo: false

ggplot(augment(model), 
       aes(x=.fitted, y=.resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_smooth(se=FALSE)+
  labs(x="fitted values of life expectancy", 
       y="model residuals")
```

:::

::::

## Its non-linear, so now what?

:::: {.columns}

::: {.column .frament}

### Transform variables
```{r}
#| echo: false
#| fig-height: 4
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  scale_x_log10(labels=scales::dollar)+
  labs(x="GDP per capita (log scale)", 
       y="Life expectancy at birth")
```

:::

::: {.column .fragment}

### Polynomial terms
```{r}
#| echo: false
#| fig-height: 4
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", 
              formula=y~x+I(x^2)+I(x^3),
              se=FALSE)+
  scale_x_continuous(labels=scales::dollar)+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")
```

:::

::: {.column .fragment}

### Create splines
```{r}
#| echo: false
#| fig-height: 4
temp <- subset(gapminder, year==2007)
cutoff <- 7500
temp$gdp.spline <- ifelse(temp$gdpPercap<cutoff, 0,
                          temp$gdpPercap-cutoff)
model <- lm(lifeExp~gdpPercap+gdp.spline, data=temp)
predict_df <- data.frame(gdpPercap=seq(from=0,to=50000,by=100))
predict_df$gdp.spline <- ifelse(predict_df$gdpPercap<cutoff, 0,
                                predict_df$gdpPercap-cutoff)
predict_df$lifeExp <- predict(model, newdata=predict_df)
ggplot(temp, 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_line(data=predict_df, color="blue", size=1)+
  scale_x_continuous(labels=scales::dollar)+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")
```

:::

::::

## Transforming variables {.smaller}

:::: {.columns}

::: {.column width = "30%"}

![](images/werewolf-transformation.jpg)

:::

::: {.column width = "70%"}

A transformation is a mathematical function that changes the value of a quantitative variable. There are many transformations that one could apply, but we will focus on one - the **log** transformation. This is the most common transformation used in the social sciences. 

Transformations are popular because they can solve multiple problems:

- A transformation can make a non-linear relationship look linear. 
- A transformation can make a skewed distribution more symmetric. 
- A transformation can reduce the impact of extreme outliers. 

:::

::::

## Plotting the log transformation

:::: {.columns}

::: {.column}

```{r}
#| eval: false
#| code-line-numbers: "5"
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  scale_x_log10(labels=scales::dollar)+#<<
  labs(x="GDP per capita (log scale)", 
       y="Life expectancy at birth")
```

- The scale of the independent variable is now multiplicative
- The relationship looks more linear now, but what does it mean?

:::

::: {.column}

```{r}
#| echo: false
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  scale_x_log10(labels=scales::dollar)+#<<
  labs(x="GDP per capita (log scale)", 
       y="Life expectancy at birth")
```

:::

::::

## The natural log {.smaller}
:::: {.columns}

::: {.column width = "30%"}


![](images/logs.png)

:::

::: {.column width = "70%"}

Although `ggplot` uses log with a base 10, we usually use the *natural log* transformation in practice. Both transformations have the same effect on the relationship, but the natural log provides results that are easier to interpret. 

::: {.fragment}

In R, it is easy to take the natural log of a number by just using the `log` command. Any positive number can be logged. 

```{r}
log(5)
```

The natural log of 5 is 1.609, but what does this mean? 

:::

::: {.fragment}

The natural log of any number is the power that you would have to raise the constant $e$ to in order to get the original number back. In R, we can calculate $e^x$ with `exp(x)`:

```{r}
exp(log(5))
```

:::

:::

::::

## logs make multiplicative relationships additive

:::: {.columns}

::: {.column width = "30%"}

![](images/lincoln_logs.jpg)

:::

::: {.column width = "70%"}

The key feature of the log transformation is that it makes multiplicative relationships additive.

$$log(x*y)=log(x)+log(y)$$

```{r}
log(5*4)
log(5)+log(4)
```

We can use this feature to model relative (percent) change rather than absolute change in our models.

:::

::::

## Logging box office returns gives us a linear fit

```{r}
#| label: fig-box-office-log
#| fig-cap: Note the logarithmic scale on the y-axis
#| echo: false
#| fig-width: 24
#| fig-height: 14

ggplot(movies, aes(x=rating_imdb, y=box_office))+
  geom_jitter(alpha=0.2)+
  geom_smooth(method="lm", color="red", se=FALSE)+
  scale_y_log10(labels = scales::dollar)+
  labs(x="IMDB Rating", y="Box Office Returns (millions)")
```

## What does it mean for the model?

To fit the model, we can just use the log transformation directly in the formula of our `lm` command:

```{r}
model <- lm(log(box_office)~rating_imdb, data=movies)
coef(model)
```

$$\log(\hat{returns}_i)=0.016+0.371(rating_i)$$

How do we interpret the slope?

::: {.fragment}

> A one point increase in IMDB rating is associated with a 0.371 increase in ... the log of box office returns? `r emo::ji("confused")`

:::

## Converting to the original scale {.smaller}

To get back to the original scale of box office returns for the dependent variable, we need to exponentiate both side side of the regression equation by $e$:

$$e^{\log(\hat{returns}_i)}=e^{0.016+0.371(rating_i)}$$

::: {.fragment}

On the left hand side, I will get back predicted box office returns by the definition of logs. On the right hand side, I can apply some of the mathematical properties of logarithms and powers. 

$$\hat{returns}_i=(e^{0.016})(e^{0.371})^{rating_i}$$

```{r}
exp(0.016)
exp(0.371)
```

:::

::: {.fragment}

I now have:
$$\hat{returns}_i=(1.016)(1.449)^{rating_i}$$

:::

## A multiplicative relationship {.smaller}

$$\hat{returns}_i=(1.016)(1.449)^{rating_i}$$

Lets calculate predicted box office returns for IMDB Ratings of 0, 1, and 2. 

::: {.fragment}

$\hat{returns}_i=(1.016)(1.449)^{0}=1.016$

:::

::: {.fragment}

$\hat{returns}_i=(1.016)(1.449)^{1}=1.449(1.449)$

:::


::: {.fragment}

$\hat{returns}_i=(1.016)(1.449)^{2}=1.449(1.449)(1.449)$

:::

- For each one unit increase in the independent variable, you multiply the previous predicted value by 1.449 to get the new predicted value. Therefore the predicted value increases by 44.9%.
- The model predicts that movies with a zero IMDB rating make 1.02 million dollars, on average. Every one point increase in the IMDB rating is associated with a 44.9% increase in box office returns, on average.

## General form and interpretation

$$\log(\hat{y}_i)=b_0+b_1(x_i)$$

- You must apply the `exp` command to your intercept and slopes in order to interpret them. 
- The model predicts that the mean of $y$ when $x$ is zero will be $e^{b_0}$. 
- The model predicts that each one unit increase in $x$ is associated with a multiplicative change of $e^{b_1}$ in $y$. It is often easiest to express this in percentage terms. 
- An **absolute** change in $x$ is associated with a **relative** change in $y$.

## Interpret these numbers {.smaller}

```{r}
coef(lm(log(box_office)~relevel(maturity_rating, "PG"), data=movies))
```

$$\log(\hat{returns}_i)=3.22+0.50(G_i)-0.24(PG13_i)-1.93(R_i)$$

- $e^{3.28}=26.58$: PG-rated movies make 26.58 million dollars on average.
- $e^{0.75}=2.12$: G-rated movies make 112% more than PG-rated movies, on average.
- $e^{-0.27}=0.76$: PG-13 rated movies make 76% as much as (or 24% less than) PG-rated movies, on average. 
- $e^{-1.79}=0.17$: R-rated movies make 17% as much as (or 83% less than) PG-rated movies, on average.

## Approximating small effects {.smaller}

When $x$ is small (say $x<0.2$), then $e^x\approx1+x$.

We can use this fact to roughly approximate coefficients/slopes as percent change when they are small.

```{r}
model <- lm(log(box_office)~runtime, data=movies)
coef(model)
exp(coef(model))
```

- If we do the full exponentiating, we can see that a one minute increase in runtime is associated with a 3.9% increase in box office returns.
- The actual percentage increase is very close to what we got for the slope of the non-exponentiated slope (0.038). So, you can often get a ballpark estimate without having to exponentiate.

## Logging the independent variable {.smaller}

:::: {.columns}

::: {.column width = "30%"}

Lets return to the relationship between GDP per capita and life expectancy that fit well as a linear relationship when we logged GDP per capita. Lets run the model:

```{r log-gdp-model}
model <- lm(lifeExp~log(gdpPercap), 
            data=subset(gapminder, 
                        year==2007))
round(coef(model), 5)
```

:::

::: {.column .fragment width = "70%"}

How do we interpret these results? This case requires something different than the case where we logged the dependent variable.

Our basic model for life expectancy by GDP per capita is:

$$\hat{y}_i=4.9+7.2\log{x_i}$$
What is the predicted value of life expectancy at $1 GDP?

$$\hat{y}_i=4.9+7.2\log{1} = 4.9+7.2 * 0=4.9$$

What happens when we increase GDP per capita by 1% (from 1 to 1.01)?

$$\hat{y}_i=4.9+7.2\log{1.01} = 4.9+7.2 * 0.01=4.9+0.072$$

Predicted life expectancy increases by 0.072 years. 

:::

::::

## The 1% increase is the same at all $x$

How much does $y$ increase when $x$ goes up by one percent?

$$(4.9+7.2\log{1.01x})-(4.9+7.2\log{x})$$
$$7.2\log{1.01x}-7.2\log{x}$$
$$7.2(\log{1.01}+\log{x})-7.2\log{x}$$

$$7.2\log{1.01}+7.2\log{x}-7.2\log{x}$$
$$7.2\log{1.01} \approx 7.2 (0.01) = 0.072$$




## General form and interpretation

$$\hat{y}_i=b_0+b_1\log(x_i)$$

- A one percent increase in $x$ is associated with a $b_1/100$ unit change in $y$, on average.
- A **relative** change in $x$ is associated with an **absolute** change in $y$.
- $exp(b_0)$ gives the predicted value of $y$ when $x$ equals one. 
- Keep in mind that the $log(0)$ is negative infinity so you cannot predict the value of $y$ when $x=0$. 

## Logging both variables

```{r}
#| label: fig-log-both
#| fig-cap: Logging both variables can sometimes solve multiple problems
#| echo: false
#| fig-width: 24
#| fig-height: 14

ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.05, width=0.1)+
  geom_smooth(se=FALSE, method="lm", linewidth = 1.5)+
  scale_y_log10(labels = scales::dollar)+
  scale_x_log10()+
  labs(x="age (log-scale)", y="hourly wages (log-scale)")
```

## The elasticity model

```{r}
#| label: elasticity-model

model <- lm(log(wages)~log(age), data=earnings)
coef(model)
```

- This is actually the easiest model to interpret. We can interpret the slope directly as the percent change in $y$ for a one percent increase in $x$. 
- Calculating the percent change in one variable by percent change in another is what economists call an **elasticity** so this model is often called an **elasticity** model.
- The model predicts that a one percent increase in age is associated with a 0.51% increase in wages, on average. 

## A cheat sheet

| Which variable logged | Non-linear shape   | Change in x | Change in y | Interpret $\beta_1$ |
|-----------------------|--------------------|-------------|-------------|---------------------|
| Independent variable  | diminishing returns| relative    | absolute    | $\beta_1/100$       |
| Dependent variable    | exponential        | absolute    | relative    | $e^{\beta_1}$       |
| Both variables        | both types         | relative    | relative    | $\beta_1$           |

---

## When $x<=0$, logging is bad {.smaller}

:::: {.columns}

::: {.column}

### Log problems

$$log(0)=-\infty$$
$$log(x)\text{ is undefined when }x<0$$

### Try the square/cube root

- The square root transformation has a similar effect to the log transformation but can include zero values. 
- The cube root transformation can also include negative values. 
- The downside of square/cube root transformations is that values are not easy to interpret. 

:::

::: {.column}

```{r}
#| label: sqrt-transform
#| echo: false
ggplot(movies, aes(x=awards, y=box_office))+
  geom_vline(xintercept = 0, linetype=2)+
  geom_jitter(alpha=0.2)+
  scale_y_log10()+
  scale_x_sqrt()+
  geom_smooth(se=FALSE)+
  labs(x="Number of Oscars (square root scale)",
       y="Box Office Returns (log scale)")
```

:::

::::

## Polynomial models  {.smaller}

:::: {.columns}

::: {.column width = "40%"}

### Polynomial expression

A **polynomial** expression is one that adds together terms involving multiple powers of a single variable. 

if we include a squared value of $x$ we get the classic formula for a parabola:

$$y=a+bx+cx^2$$

```{r}
#| echo: false
#| fig-height: 6


x <- 0:100
y <- 10+5*x-0.04*x^2
plot(x,y, type="l", lwd=2, las=1)
text(40, 50, expression(y == 10 + 5*x + 0.04*x^2))
```

:::

::: {.column .fragment width = "60%"}

### A polynomial model

We can fit such a parabola in a linear model by including a new variable that is simply the square of the original variable:

$$\hat{y}_i=\beta_0+\beta_1x_i+\beta_2x_i^2$$

```{r}
#| label: quadratic-model

model <- lm(wages~I(age-40)+I((age-40)^2), 
            data=earnings)
coef(model)
```

Our model is:

$$\hat{y}_i=26.9+0.3171(x_i-40)-0.0178(x_i-40)^2$$

How do we interpret the results?

:::

::::

## Calculate the marginal effect {.smaller}

With polynomial terms, the marginal effect of $x$ is more complicated because our $x$ shows up in more than one place in the equation:

$$y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\epsilon_i$$

::: {.fragment}

By calculus, we can mathemagically get the marginal effect for this model as:

$$\frac{\partial y}{\partial x}=\beta_1+2*\beta_2x$$
:::

::: {.fragment}

So, for our case, the marginal effect of age on wages is given by:

$$0.3171+2*-0.0178(x-40)=0.3171-0.0356(x-40)$$

- At age 40 (the zero value), a one year increase in age is associated with a salary increase of $0.32, on average. 
- For every year over 40, this increase is smaller by $0.0356. For every age younger than 40, this increase is larger by $0.0356.

:::

## Finding the inflection point {.smaller}

:::: {.columns}

::: {.column}

If the effect of age on wages goes down by $0.0356 for every year over 40, then at some point the positive effect of age on wages will become negative. 

We can figure out the value of age at this **inflection point** by setting the effect to zero and solving for x. In general, this will give us:

$$\beta_1/(-2*\beta_2)$$

In our case, we get: 

$$0.3171/(-2*-0.0178)=8.91$$

So the model predicts that the effect of age on wages will shift from positive to negative at age 48.91.

::: 

::: {.column}

```{r}
#| echo: false
age <- 18:65
marginal <- 0.3171-0.0356*(age-40)
ggplot(data.frame(age=age, marginal=marginal),
       aes(x=age, y=marginal))+
  geom_line()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_vline(xintercept = 48.91, color="red")+
  labs(x="age",
       y="marginal effect of age on hourly wages",
       title="marginal effect graph")
```

:::

::::

## Plotting a polynomial fit

:::: {.columns}

::: {.column}

```{r}
#| eval: false
#| code-line-numbers: "4-5"

ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width=1)+
  geom_smooth(se=FALSE, 
              method="lm",
              formula=y~x+I(x^2))+ 
  labs(x="age", y="hourly wages")
```

in `geom_smooth`, you can specify a formula for the `lm` method that includes a squared term.

:::

::: {.column}

```{r}
#| echo: false

ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width=1)+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2))+ 
  labs(x="age", y="hourly wages")
```

:::

::::

## Higher order terms gives you more wiggle {.smaller}

:::: {.columns}

::: {.column}

```{r}
#| eval: false
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2),
              color="blue")+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2)+I(x^3),
              color="red")+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2)+I(x^3)+I(x^4),
              color="green")+
    labs(x="GDP per capita", 
         y="life expectancy")
```

:::

::: {.column}

```{r}
#| echo: false
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2),
              color="blue")+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2)+I(x^3),
              color="red")+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2)+I(x^3)+I(x^4),
              color="green")+
    labs(x="GDP per capita", 
         y="life expectancy")
```



:::

::::

---

## Spline models {.smaller}

:::: {.columns}

::: {.column}

```{r}
#| label: fig-spline-wages
#| fig-cap: Spline models create hinges or "broken arrows"
#| echo: false

earnings$age.spline <- ifelse(earnings$age<35, 0, earnings$age-35)
model <- lm(wages~age+age.spline, data=earnings)
predict_df <- data.frame(age=18:65)
predict_df$age.spline <- ifelse(predict_df$age<35, 0, predict_df$age-35)
predict_df$wages <- predict(model, newdata=predict_df)
ggplot(earnings, aes(x=age, y=wages))+
geom_jitter(alpha=0.01, width = 1)+
  geom_line(data=predict_df, size=1.5, color="blue")+
  geom_vline(xintercept = 35, color="red", linetype=2)+
  labs(y="hourly wages",
       title="spline model with hinge at age 35")
```

:::

::: {.column}

- The basic idea of a spline model is to allow the slope of the relationship between $x$ and $y$ to be different at different cutpoints or "hinge" values of $x$. 
- These cutpoints create different linear segments where the effect of x on y is different. 
- We will look at the case of one hinge value which gives us an overall slope that looks like a "broken arrow."

:::

::::

## Creating the spline variable

The relationship between age and wages suggests that the relationship shifts considerably around age 35. To model this we create a spline variable like so:

$$spline_i=\begin{cases}
  age-35 & \text{if age>35}\\
  0 & \text{otherwise}
  \end{cases}$$

::: {.fragment}

We can then add this variable to our model:
  
```{r}
#| label: create-spline
earnings$age.spline <- ifelse(earnings$age<35, 0, earnings$age-35)
model <- lm(wages~age+age.spline, data=earnings)
coef(model)
```

How do we interpret the results?

:::

## Interpreting results in spline model {.smaller}

:::: {.columns}

::: {.column}

Up to age 35, the spline term is zero, so our model is given by:

$$\hat{wages}_i=-6.04+0.9472(age_i)$$

The model predicts that for individuals age 35 and under, a one year increase in age is associated with a $0.9472 increase in wages, on average. 

After age 35, the spline variable increases by one every time age increases by one, so the marginal effect of age is given by:

$$0.9472-09549=-0.0077$$

The model predicts that for individuals over age 35, a one year increase in age is associated with a $0.0077 reduction in wages, on average.

:::

::: {.column}

```{r}
#| echo: false
ggplot(earnings, aes(x=age, y=wages))+
geom_jitter(alpha=0.005, width = 1)+
  geom_line(data=predict_df, size=1.5, color="blue")+
  geom_vline(xintercept = 35, color="red", linetype=2)+
  labs(y="hourly wages")+
  annotate("text", x=28, y=30, 
           label=paste("beta == ", 0.9472), 
           parse=TRUE,
           color="blue")+
  annotate("text", x=50, y=35, 
           label=paste("beta == ", -0.0077), 
           parse=TRUE,
           color="blue")
```

:::

::::

## {{< fa user-ninja >}} Plotting the spline model {.smaller}

:::: {.columns}

::: {.column}

We cannot use `geom_smooth` to show the fit of this spline model. However, with some additional effort, we can plot the effect on a graph. 

1. Calculate the predicted value of wages for a range of age values based on the model using the `predict` command.
2. Feed in these predicted values as a dataset to the `geom_line` function in our `ggplot` graph.

:::

::: {.column .fragment}

### The `predict` command

The `predict` command takes two important arguments:

1. The model object for which we want predicted values of the dependent variable.
2. A new dataset that contains all the same independent variables that are in the model.

::: {.fragment}

```{r}
#| label: use-predict-spline

model <- lm(wages~age+age.spline, data=earnings)
pre_df <- data.frame(age=18:65)
pre_df$age.spline <- ifelse(pre_df$age<35, 0,
                            pre_df$age-35)
pre_df$wages <- predict(model, 
                        newdata=pre_df)
head(pre_df, n=3)
```

:::

:::

::::

## Adding the spline fit to ggplot

:::: {.columns}

::: {.column}


```{r}
#| eval: false
#| code-line-numbers: "3"

ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width = 1)+
  geom_line(data=predict_df,
            color="blue", 
            size=1.5)
```

To add the spline fit, we just add a `geom_line` command and specify our `pre_df` dataset through the `data` argument so that it uses the predicted values rather than the actual `earnings` dataset to graph the line.

:::

::: {.column}

```{r}
#| echo: false

ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width = 1)+
  geom_line(data=predict_df, #<<
            color="blue", 
            size=1.5)
```

:::

::::

# The IID Violation and Robust Standard Errors {.center background-color="black" background-image="images/bence-balla-schottner-o5ttfkZU9_Q-unsplash.jpg"}

##  Linear model as data-generating process {.smaller}

$$y_i=\underbrace{\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}}_{\hat{y}_i}+\epsilon_i$$

- To get values of $y_i$, you feed in values of $x_i$ to the structural component and get back out a predicted $\hat{y_i}$ value.
- To get the stochastic (random) part, you then reach into some distribution to grab a random value of $\epsilon_i$ that you add to your predicted value to get an actual value of $y_i$.
- What distribution are you reaching into when you grab $\epsilon_i$?
    - **Independence**: The number you pull out each time doesn't depend on other numbers that you pull out. This is most commonly violated by **autocorrelation** or the clustering of **repeated observations**.
    - **Identical distribution**: You reach into the same distribution for all observations. This is most commonly violated by **heteroscedasticity** which means non-constant variance in the residuals.
- Together these assumptions give us the **IID** assumption of the linear model: the error terms are **independent and identically distributed**.

## Violations of independence {.smaller}

:::: {.columns}

::: {.column .fragment}

### Serial autocorrelation

```{r}
#| label: time-series
#| echo: false
#| fig-height: 7

ggplot(births_nyc, aes(x = date, y = births))+
  geom_line()+
  geom_smooth(method = "lm", se = FALSE)+
  scale_x_date(breaks = seq(from = date("1948-01-01"), to = date("1959-01-01"), by = "year"),
               labels = paste(1948:1959))+
  labs(x = "date", y = "monthly birth rate in NYC")+
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

In time series data, sequential observations in time are likely to either be highly positively or negatively correlated. In this case, we can see clear seasonal fluctutation in birth rates.

:::

::: {.column .fragment}

### Repeated observations

![](images/classroom.jpg)

When observations are drawn repeatedly from a sample of some larger units (i.e. a multilevel structure), then observations within the same unit are likely to vary from predicted values in the same way. For example, students in the same classroom might tend to have either lower or higher test scores than predicted by the model due to some unobserved feature of that classroom.

:::

::::

##  Serial autocorrelation example {.smaller}

:::: {.columns}

::: {.column}

```{r}
#| eval: false
model <- lm(births~month, data=births_nyc)
augment(model) |>
  ggplot(aes(x = .resid, 
             y = lag(.resid, 1)))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE)+
  geom_hline(yintercept = 0, linetype=2)+
  labs(x="residuals",
       y="residuals (lagged one month")
```

As an example of serial autocorrelation, I will use the `longley` time series dataset in R to fit the following model predicting the number of people employed by GNP from 1947 to 1962 (n=16):

I can then plot the residuals values from years 1947 to 1961  by the residual values for years 1948 to 1962. The positive correlation in the residuals here suggests serial autocorrelation.

:::

::: {.column}

```{r}
#| echo: false
model <- lm(births~month, data=births_nyc)
augment(model) |>
  ggplot(aes(x = .resid, 
             y = lag(.resid, 1)))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE)+
  geom_hline(yintercept = 0, linetype=2)+
  labs(x="residuals",
       y="residuals (lagged one month")+
  annotate("text", -2, 2, label="r=0.31", size = 2)
```

:::

::::

## Heteroscedasticity {.smaller}

:::: {.columns}

::: {.column}

![](images/mary_poppins.jpg)

- Heteroscedasticity means that the variance of the residuals is not constant but depends on the values of $x_i$, and therefore, implicitly, $\hat{y}_i$. 
- A classic example of heteroscedasticity is when the variance of the residuals increases with larger values of $\hat{y}_i$ giving you a cone shape in a residual by fitted value plot. 

:::

::: {.column .fragment}

```{r}
#| label: fig-heteroscedasticity
#| fig-cap: The variance of the residuals increases with the predicted value
#| echo: false

model <- lm(box_office~rating_imdb, data=movies)
segments <- data.frame(x=c(0,25,50, 75),
                       y=c(0,-20,-60, -90),
                       xend=c(0,25,50, 75),
                       yend=c(200,300,440, 650))
ggplot(augment(model), aes(x=.fitted, y=.resid))+
  geom_point(alpha=0.7)+
  geom_hline(yintercept=0, linetype=2)+
  geom_segment(data=segments,
               aes(x=x, y=y, xend=xend, yend=yend),
               color="red", size=2,
               arrow = arrow(length = unit(0.03, "npc"), end="both"),
               alpha=0.5)+
  labs(x="fitted values of box office returns by IMDB rating",
       y="model residuals")
```

:::

::::

##  Correcting for iid violations {.smaller}

Violating the iid assumption does not bias your results, but it will lead to inefficient estimates and poorly estimated standard errors. 

There are a number of potential solutions to the iid problem. These include:

- **Transformations** (particularly the log transformation) can often solve the problem of heteroscedasticity. 
- **Weighted least squares** models can correct for iid when the nature of the violation is understood.
- **Robust standard errors** can be used as a crude brute-force solution when the nature of the violation is not well understood. I would recommend that this only be done for diagnostic reasons. 
- In general, the best approach is to re-think your model. If you have an iid violation then you are probably not applying the best type of model to the problem at hand. 

##  Fixing heteroscedasticity with a transformation

:::: {.columns}

::: {.column}

```{r}
#| echo: false

model <- lm(box_office~rating_imdb, data=movies)
ggplot(augment(model), aes(x=.fitted, y=.std.resid))+
  geom_mark_hull(fill = "grey30", alpha = 0.2, color = NA, concavity = 3)+
  geom_jitter(alpha=0.7)+
  geom_hline(yintercept=0, linetype=2)+
  geom_smooth(se = FALSE)+
  labs(x="fitted values of box office returns by IMDB rating",
       y="model residuals",
       title="Original scale")
```

:::

::: {.column .fragment}

```{r}
#| echo: false

model <- lm(log(box_office)~rating_imdb, data=movies)
ggplot(augment(model), aes(x=.fitted, y=.std.resid))+
  geom_mark_hull(fill = "grey30", alpha = 0.2, color = NA, concavity = 3)+
  geom_jitter(alpha=0.7)+
  geom_hline(yintercept=0, linetype=2)+
  geom_smooth(se = FALSE)+
  labs(x="fitted values of log box office returns by IMDG Rating",
       y="model residuals",
       title="After logging box office returns")
```

:::

::::

##  Using Weighted Least Squares

The weighted least squares technique uses a weighting matrix $\mathbf{W}$ in its calculation of regression slopes like so:


$$\mathbf{\beta}=(\mathbf{X'W^{-1}X})^{-1}\mathbf{X'W^{-1}y}$$
$$SE_{\beta}=\sqrt{\sigma^{2}(\mathbf{X'W^{-1}X})^{-1}}$$

The exact form of this weighting matrix depends on the nature of the iid violation, but in general it is used to represent the covariance between residuals. 

- Values in the diagonal cells adjust for heteroscedasticity.
- Values in other cells adjust for autocorrelation.

##  GLS Example {.smaller}

We can use the `gls` command in the `nmle` package to adjust for serial autocorrelation in the prior model.We will assume that the autocorrelation follows an "AR2" pattern in which each subsequent residual is correlated with its two immediate predecessors (AR2 stands for auto-regressive 2, where 2 indicates the lag). 

```{r}
library(nlme)
summary(lm(births~month, data=births_nyc))$coef
model_ar2 <- gls(births~month, correlation = corARMA(form=~month, p = 2),
                 data = births_nyc)
summary(model_ar2)$tTable
```

## Robust Standard Errors {.smaller}

:::: {.columns}

::: {.column}

We won't delve into the math behind the robust standard error, but the general idea is that robust standard errors will give you "correct" standard errors even when the model is mis-specified due to issues such a non-linearity, heteroscedasticity, and autocorrelation.

Robust standard errors can be estimated in R using the `sandwich` and `lmtest` packages, and specifically with the `coeftest` command. Within this command, it is possible to specify different types of robust standard errors, but we will use the "HC1" version which is equivalent to the robust standard errors produced in Stata by default. 

:::

::: {.column .fragment}

```{r}
#| label: robust-se

library(sandwich)
library(lmtest)
model <- lm(box_office~rating_imdb, data = movies)
summary(model)$coef[,1:2]
coeftest(model, vcov = vcovHC(model, "HC1"))[,1:2]
```


::: {.fragment}

The estimates are the same, but the robust standard errors are considerably larger. That difference in magnitude is telling us that our basic regression model is problematic. In this case, we already know that the problem is heteroscedasticity.

:::

:::

::::

##  `r emo::ji("warning")` Robust standard errors diagnose problems that they do not fix! {.smaller}

Lets use robust standard errors on a model where we first log box office returns:

::: {.fragment}

```{r}
#| label: robust-se-log

model <- lm(log(box_office)~rating_imdb, data = movies)
summary(model)$coef
coeftest(model, vcov = vcovHC(model, "HC1"))
```

:::

::: {.fragment}

When the dependent variable is logged to remove heteroscedasticity, the difference between robust and regular standard errors goes away.

:::


# Sample Design and Weighting {.center background-color="black" background-image="images/cyril-saulnier-TsVN31Dzyv4-unsplash.jpg"}

##  The reality of survey sampling {.smaller}

:::: {.columns}

::: {.column}

### The simple random sample

![](images/srs.png)

- In a simple random sample (SRS) of size $n$, every possible combination of $n$ observations from the population has an equally likely chance of being drawn.
- Eevery statistic from an SRS should be representative of the population, except for random sampling bias.

:::

::: {.column .fragment}

### Reality

![](images/strat_sample.png)

- In practice, large-scale surveys never use SRS for pragmatic and design reasons. 
- For correct statistical inference, we typically need to make adjustments for sample design.
- The primary ways in which sample design affects estimation are **clustering**, **stratification**, and **weighting**.

:::

::::

##  Cluster/Multistage sampling {.smaller}

:::: {.columns}

::: {.column width = "30%"}

![](images/multistage.png)
:::

::: {.column width = "70%"}

Potential observations are aggregated into larger groupings identified as the Primary Sampling Unit (PSU) and then we sample some of these PSUs before sampling individual observations within the sampled PSUs.

- Cluster sampling is about efficiency and cost. Clusters are typically defined geographically, which then minimizes the cost of sampling individual observations. 
- If PSUs are sampled with probabilities proportional to cluster size, then every unit in the population has an equal likelihood of being selected. In this case, summary statistics on the sample should be representative. 
- When the variable of interest is distributed differently across clusters, the sampling variability will be higher than an SRS even if every observation has an equally likely chance of being drawn. 

:::

::::

##  What percent of the US population is Mormon? {.smaller}

:::: {.columns}

::: {.column}
The General Social Survey uses Metropolitan Statistical Areas (MSA) and clusters of non-metropolitan counties as the PSU. In each year, it draws a sample of these areas and then samples respondents within each sampled PSU.

Because Mormons are heavily concentrated in certain places, percent Mormon in the GSS varies substantially from year to year by whether certain PSUs were sampled or not. Red band indicates expected 95% interval for sampling variability without clustering, assuming the average percent across all years (dotted line).

:::

::: {.column}

```{r}
#| label: fig-mormons
#| fig-cap: The variance in the proportion across years is higher than we would expect if we were drawing an SRS
#| echo: false

temp <- relig |>
  filter(!is.na(relig))

prop_mormon_pooled <- mean(!is.na(temp$other) & temp$other == "mormon")

temp |>
  group_by(year) |>
  summarize(prop_mormon = mean(!is.na(other) & other == "mormon"),
            n = n()) |>
  ungroup() |>
  mutate(se = sqrt(prop_mormon_pooled * (1-prop_mormon_pooled) / n),
         highmark = prop_mormon_pooled + 1.96 * se,
         lowmark = prop_mormon_pooled - 1.96 * se) |>
  ggplot(aes(x = year, y = prop_mormon))+
  geom_ribbon(aes(ymin = lowmark, ymax = highmark), alpha = 0.4, fill = "red")+
  geom_line()+
  geom_point()+
  geom_hline(yintercept = prop_mormon_pooled, linetype=2)+
  annotate("text", 1980, 0.015, 
           label="expected 95% confidence\ninterval under SRS",
           color="red")+
  scale_y_continuous(labels = scales::percent)+
  labs(x="year",
       y="sample percent Mormon",
       caption="Data: General Social Survey")
```

:::

::::

##  Stratification {.smaller}

Stratification in sampling operates in a manner somewhat similar to cluster sampling except that once the observations are aggregated into strata by some characteristic (e.g. income, race, age), observations are sampled from *every* stratum. 


- Stratification is typically done to ensure that various sub-populations are present within the sample. 
- Different strata may be sampled with different probabilities. The most common approach is to take an **oversample** of a small group in order to ensure that effective comparisons can be made between that group and other groups.
- In practice, stratification is often done by first screening potential respondents for stratum characteristics.
- If strata are sampled with different probabilities, then summary statistics for the full sample will not be representative without weight adjustments. 
- Unlike clustering, greater similarity on a characteristic of interest within strata can actually **reduce** the sampling variability for that characteristic relative to an SRS. 

##  Weighting {.smaller}

- Numerous factors can lead to a sample being unrepresentative such as sampling strata with different probabilities, differential non-response rates, and a lack of fit between sample frame and population.
- **Sampling weights** allow researchers to correct summary statistics from the sample so that they are representative of the population. 
- The sampling weight for an observation should be $1/p_i$ where $p_i$ is the probability of being sampled. The sampling weight indicates the number of observations in the population that observation $i$ in the sample represents.
- Calculating sampling weights can be quite complex. In some cases, researchers may know $p_i$ from the study design. In other cases, researchers may create **post-stratification** weights by comparing the sample to some other data source (e.g. census, school records) for a set of demographic characteristics and applying weights to make the sample align with the other data source.
- When sampling weights are present in a dataset, they must be used to generate statistics representative of the population.
- Variation in sampling weights will increase sampling variability above and beyond that expected for an SRS.

##  Weights in Sneetchville {.smaller}

:::: {.columns}

::: {.column width = "30%"}

![sneetches](images/sneetches.jpg)

:::

::: {.column width = "70%"}

In Sneetchville, there are 3 star-bellied and 7 regular sneetches. Lets say I take a stratified sample of two star-bellied and two regular sneetches. Here is the population data:

```{r}
#| label: make-sneetches
sneetches <- tibble(type = factor(c(rep("Star-bellied", 3), rep("Regular", 7))),
                    income = c(3, 7, 6, 2, 1, 4, 5, 0, 2, 1),
                    prob = c(rep(2/3, 3), rep(2/7, 7)))
gt(sneetches)
```

:::

::::

##  Weights in Sneetchville 


```{r}
#| label: sample-sneetches

sneetch_sample <- sneetches[c(sample(1:3, 2), sample(4:10, 2)),]
sneetch_sample$weight <- 1/sneetch_sample$prob
gt(sneetch_sample)
mean(sneetches$income)
mean(sneetch_sample$income)
weighted.mean(sneetch_sample$income, sneetch_sample$weight)
sum(sneetch_sample$income*sneetch_sample$weight)/sum(sneetch_sample$weight)
```

##   The consequences of survey design {.smaller}


| Design issue     |  Representative?                                                                |  Design effect (change to SE)               | 
|:-----------------|:--------------------------------------------------------------------------------|:--------------------------------------------|
| Clustering       | Yes, if PSUs are proportionally drawn. Otherwise, weighting necessary.          | Increases with difference between clusters. |
| Stratification   | Yes, if strata are sampled with same probability. Otherwise weighting necessary.| Decreases with homogeneity within strata    |
| Weights          | Only if weights are applied.                                                    | Increases with the variance of the weights. |

##  Correcting for survey design 

::: {.fragment}

### Basic Weighting

R has syntax in many commands to apply sampling weights to get representative statistics (e.g. `weighted.mean`, the `weight` option in the `lm` command), but this approach will not correctly adjust standard errors for design effects. 

:::

::: {.fragment}

### Robust standard errors

Robust standard errors will correct standard errors for differential weights, but not for clustering and stratification design effects. 

:::

::: {.fragment}

### Survey package

The `survey` package in R will allow you to specify design and correctly adjust standard errors. 

:::

## Add Health survey design {.smaller}

:::: {.columns}

::: {.column width = "30%"}

![](images/addhealthdesign.jpeg)

:::

::: {.column width = "70%"}

- Schools were the primary PSU in a cluster sampling technique, but were also stratified by region, urbanicity, school type, ethnic mix, and size. 
- Students within schools were stratified by grade and sex and then sampled.
- Several oversamples were conducted of ethnic groups and genetically related pairs of students, as well as saturated samples from 16 schools. 
- Post-stratification adjustments were made to sampling weights to account for region of the country. 
- The Add Health documentation indicates that the REGION variable should be applied as a stratification variable but this variable is not available in the public release data so we will focus on the design effects of weights and clustering. 

:::

::::

##  Sample design variables {.smaller}

:::: {.columns}

::: {.column}

```{r}
#| label: addhealth-design-var

addhealth |>
  select(cluster, sweight)
```
- `cluster` is a numeric id indicating the school the student was sampled from.
- `sweight` is the inverse of the probability of being sampled.

:::

::: {.column .fragment}

```{r}
#| label: fig-addhealthweight-dist
#| fig-cap: Some observations count more than others
#| echo: false

ggplot(addhealth, aes(x=sweight))+
  geom_histogram(fill="darkgreen", color="grey20")+
  labs(x="sampling weight",
       title="Sampling weight distribution in Add Health data")
```

:::

::::

##  Add Health example, naive approach {.smaller}

:::: {.columns}

::: {.column}

### Basic model

```{r}
#| label: model-no-weight

model_basic <- lm(nominations~nsports, data = addhealth)
summary(model_basic)$coef
```

- Estimates will be biased by differential probability of being sampled
- Standard errors are underestimated due to:
   - variability in sampling weights
   - the use of a cluster sampling design

:::

::: {.column .fragment}

### Adjusting for weights

I can add the weights provided by Add Health to make this representative:

```{r}
#| label: model-weights

model_weight <- update(model_basic, weight = sweight)
summary(model_weight)$coef
```

- The slope estimates in this model are unbiased but I have not accounted for weight variability in my standard error estimates.

:::

::::

##  Add Health example, robust SE approach {.smaller}

I can correct for sampling weight variability by using robust standard errors:

```{r}
#| label: model-weight-robust

library(sandwich)
library(lmtest)
model_robust <- coeftest(model_weight, vcov = vcovHC(model_weight, "HC1"))
model_robust
```

- Estimates are identical to the weighted `lm` but standard errors are slightly larger. However, this technique still does not correct for clustering.

##  Add Health example, using `survey` library {.smaller}

I can use the `svydesign` command in the `survey` package to correctly specify both the weights and the clustering in Add Health. The `ids` argument expects a variable name that identifies the clusters by id (in this case, the school id) and the `weight` argument expects a variable name for the weights used. 

```{r}
#| label: survey-package-example

library(survey)
addhealth_svy <- svydesign(ids = ~cluster, weight = ~sweight, data = addhealth)
model_svy <- svyglm(nominations~nsports, design = addhealth_svy)
summary(model_svy)$coef
```

- Estimates are identical to the weighted model before, but standard errors have increased due to variation in weights **and** the cluster design effect. 

##  Comparison of methods {.smaller}

```{r}
#| label: tbl-sdesign-compare
#| tbl-cap: Comparison of modeling techniques for dealing with sampling design
#| echo: false

addhealth_svy_jweight <- svydesign(ids = ~1, weight = ~sweight, data = addhealth)
model_svy_jweight <- svyglm(nominations~nsports, design=addhealth_svy_jweight)
modelsummary(list("unweighted OLS" = model_basic, 
                  "weighted OLS" = model_weight, 
                  "robust SE" = model_robust, 
                  "svy: weights" = model_svy_jweight, 
                  "svy: weights+cluster" = model_svy),
             output = "gt",
             fmt = 3,
             stars = TRUE,
             coef_map = c("(Intercept)" = "Intercept",
                          "nsports" = "Number of sports played"),
             gof_map = c("nobs", "r.squared"))
```


- All models except the unweighted version produce the same estimates of slope and intercept based on the weights.
- The survey weighted and robust SE models both produce the same standard errors. This is because both models account for the design effect of weight variance but not clustering. 

# Missing Data {.center background-color="black" background-image="images/kyaw-tun-UvL9JrLkagM-unsplash.jpg"}

##  The reality of missing data {.smaller}

Missing data exists in most real-world data sets. Therefore, its important to know how to handle missing data in order to know how to properly conduct an analysis.

:::: {.columns}

::: {.column .fragment}

### Valid Skip

![](images/baby_boss.jpg)

Valid skips most arise when a follow-up question is only asked of respondents who gave a certain response to the initial question. If you construct a variable correctly, valid skips should not be considered missing values.

:::

::: {.column .fragment}

### Item non-response

![](images/slamming-door-shutting-door.gif)

Item non-response occurs when respondents fail to respond to a specific question. This may be because they don't know the correct response or they do not feel comfortable answering the question. 

:::

::::

##  Example of a valid skip {.smaller}

The General Social Survey uses three variables to determine respondents' religious affiliation. 

- `relig` asks for major religious affiliations such as Catholic, Protestant, Jewish, Muslim, etc.
- **If and only if** respondents indicate they are Protestant, they are asked a follow up question recorded in `denom` which asks for their specific denomination. 
- `denom` only lists major Protestant denominations. If the respondent checks "other", their specific write-in response is recorded in a third variable titled `other`. 

::: {.fragment}

```{r}
#| label: gss-denom

relig |> select(relig, denom, other) |> summary()
```

:::

::: {.fragment}

There are a lot of missing values for `denom` and `other`, but these are all valid skips based on prior responses. The only true missing values in this set of variables are the 23 respondents who did not respond to the initial question on `relig`.  

:::

##  Kinds of missingness {.smaller}

:::: {.columns}

::: {.column width = "25%"}

![](images/coffee_spill.jpg)

:::

::: {.column width = "75%"}

::: {.fragment}

### Missing Completely at Random (MCAR)
Every observation has the same probability of missingness and the missingness of a variable has no relationship to other observed or unobserved variables. If this is true, then removing observations with missing values will not bias results.

:::

::: {.fragment}

### Missing at Random (MAR)

The different probabilities of missingness can be fully accounted for by other observed variables in the dataset. If this is true, then various techniques can be used to produce unbiased results by **imputing** values for the missing values. 

:::

::: {.fragment}

### Not Missing at Random (NMAR)

The different probabilities of missingness depend both on observed and unobserved variables. In this case, we cannot fully correct for bias that might result from missing data. 

:::

:::

::::

##  Add Health income example

As an example, I will use parental income from the Add Health data to predict popularity. Income is recorded in thousands of dollars, and I have top-coded the values to $200,000. Income is notorious as a variable that will typically have a high non-response rate. The Add Health data are no different:

```{r addhealth_incomesummary}
#| label: addhealth-income-summary

summary(addhealth$parent_income)
mean(is.na(addhealth$parent_income))
```

Income is missing for 1027 cases which is roughly a quarter of the dataset. 

## Two basic approaches to missing values {.smaller}

:::: {.columns}

::: {.column .fragment}

![](images/throw_away.png)

### Remove cases

- Case deletion is easy to implement.
- Case deletion assumes MCAR.
- Case deletion may result in a substantial reduction is sample size.

:::

::: {.column .fragment}

![](images/rabbit.jpg)

### Impute cases

- Imputation is more difficult to implement.
- If done properly, imputation assumes MAR.
- Imputation will not reduce sample size.

:::

::::

##  Two kinds of case deletion {.smaller}

:::: {.columns}

::: {.column .fragment}

### Complete-case analysis (listwise deletion)

- All cases that have missing values on any of the variables that will be used at some point in the analysis are removed from the start. 
- This ensures that all results are based on the same sample.

:::

::: {.column .fragment}

### Available-case analysis (pairwise deletion)

- Cases are removed model by model or statistic by statistic when the variables used in that particular statistic or model have missing values. 
- This is by default what will happen in R across different `lm` models with different variables.
- This approach allows the researcher to use more data, but different statistics and models will use different subsets of the full data which makes comparability problematic. 

:::

::::

##  Removing cases, Add Health example  {.smaller}

:::: {.columns}

::: {.column .fragment}

### Available-case analysis

This is what happens by default when you just run nested models in R

```{r}
#| label: models-available-case

model1.avail <- lm(nominations~nsports, data = addhealth)
model2.avail <- update(model1.avail, .~.+alcohol_use+smoker)
model3.avail <- update(model2.avail, .~.+parent_income)
```

:::

::: {.column .fragment}

### Complete-case analysis

To do this, we need to use `drop_na` on the Add Health variables that will be in the most complex model to make sure all models work with the same subset. 

```{r}
addhealth_complete <- addhealth |>
  select(nominations, nsports, alcohol_use, smoker, 
         parent_income) |>
  drop_na()
model1.complete <- lm(nominations~nsports,  
                      data = addhealth_complete)
model2.complete <- update(model1.complete, 
                          .~.+alcohol_use+smoker)
model3.complete <- update(model2.complete, 
                          .~.+parent_income)
```

:::

:::

##  What is different?

:::: {.columns}

::: {.column}

```{r}
#| label: tbl-available-results
#| tbl-cap: Models predicting number of friend nominations, available cases
#| echo: false

cm <- c("(Intercept)" = "Intercept",
        "nsports" =  "Number of sports",
        "alcohol_useDrinker" = "Drinker",
        "smokerSmoker" = "Smoker",
        "parent_income" = "Parental income (1000s USD)",
        "parent_income_missingTRUE" = "Parent income imputed",
        "parent_income.meani" = "Parental income (1000s USD)",
        "parent_income.chosen" = "Parental income (1000s USD)") 

modelsummary(list(model1.avail, model2.avail, model3.avail),
        coef_map = cm,
        output = "gt",
        stars = TRUE,
        gof_map = c("nobs", "r.squared"))
```
:::

::: {.column}

```{r}
#| label: tbl-complete-case
#| tbl-cap: Models predicting number of friend nominations, complete cases
#| echo: false
modelsummary(list(model1.complete, model2.complete, model3.complete),
        coef_map = cm,
        output = "gt",
        stars = TRUE,
        gof_map = c("nobs", "r.squared"))
```

:::

::::

##  Imputation {.smaller}

:::: {.columns}

::: {.column .fragment}

### Predictive or non-Predictive?

- Imputations vary by whether or not they use other predictors in the data to assign imputed values to missing values. 
- Predictive imputation is done via a statistical model. 
- Predictive imputation moves the assumption from MAR to MCAR if the model is good.
- Non-predictive imputation will systematically bias correlation between variables downward.

:::

::: {.column .fragment}

### Inject Randomness?

- Imputations vary by whether or not they include a random component or are purely deterministic. 
- Deterministic imputation techniques will underestimate variance in the imputed variable and will therefore underestimate standard errors. 
- The use of randomization leads to another source of variation called **imputation variation** which can only be fully addressed through the technique of **multiple imputation**.

:::

::::

##  Non-Predictive Imputation

A very simple (and poor) technique would be just to substitute the mean for valid responses for all missing values. 

```{r}
#| label: mean-impute
addhealth <- addhealth |>
  mutate(parent_income_missing = is.na(parent_income),
         parent_income.meani = ifelse(parent_income_missing, 
                                     mean(parent_income, na.rm = TRUE),
                                     parent_income))
```

::: {.fragment}

Another similar technique that allows for more randomness is just to sample a random valid response on the same variable for each missing value. 

```{r}
#| label: random-impute

addhealth$parent_income.randi <- addhealth$parent_income
random_values <- sample(na.omit(addhealth$parent_income), 
                        sum(is.na(addhealth$parent_income)))
addhealth$parent_income.randi[is.na(addhealth$parent_income)] <- random_values
```

:::

##  Mean and random imputation, Add Health

:::: {.columns}

::: {.column}

```{r}
#| label: plot_mean_impute
#| echo: false

ggplot(addhealth, aes(x=parent_income.meani, y=nominations, 
                      color=parent_income_missing))+
  geom_point(alpha=0.6)+
  scale_color_manual(values=c("grey20","red"))+
  labs(x="parental income ($1000s)", y="number of friend nominations",
       color="Income value imputed")+
  theme(legend.position = "bottom")
```

:::

::: {.column .fragment}

```{r}
#| label: plot_random_impute
#| echo: false

ggplot(addhealth, aes(x=parent_income.randi, y=nominations, 
                      color=parent_income_missing))+
  geom_point(alpha=0.6)+
  geom_smooth(method="lm", se=FALSE)+
  scale_color_manual(values=c("grey20","red"))+
  labs(x="parental income ($1000s)", y="number of friend nominations",
       color="Income value imputed")+
  theme(legend.position = "bottom")
```

:::

::::

##  Non-predictive imputation == Bad

| Sample              | $r$ (nominations and income) | SD (income)        |
|:--------------------|--------------------------:|-------------------:|
| Valid cases         |  `r round(cor(addhealth$parent_income, addhealth$nominations, use="complete.obs"), 3)` | `r round(sd(addhealth$parent_income, na.rm=TRUE), 1)`|
| Valid cases +mean imputed |  `r round(cor(addhealth$parent_income.meani, addhealth$nominations, use="complete.obs"), 3)` | `r round(sd(addhealth$parent_income.meani, na.rm=TRUE), 1)`|
| Valid cases +random imputed |  `r round(cor(addhealth$parent_income.randi, addhealth$nominations, use="complete.obs"), 3)` | `r round(sd(addhealth$parent_income.randi, na.rm=TRUE), 1)`|

- Both techniques will systematically underestimate correlation.
- Mean imputation will underestimate the variance of the imputed variable. 

##  Quick and dirty method {.smaller}

If I do a mean imputation, I can also add a boolean variable for missingness as a predictor in my model. The effect of the imputed variable will be the same as if if I had thrown out missing values (because we are controlling for missingness), but I get to use the full data. 

```{r}
#| label: tbl-missing-dummy
#| tbl-cap: Models predicting friend nominations
#| echo: false
modelsummary(
  list(
    "Drop missing" = lm(nominations~parent_income, data=addhealth),
    "Missing dummy" = lm(nominations~parent_income.meani+parent_income_missing, data=addhealth)
  ),
  coef_map = cm,
  output = "gt",
  stars = TRUE,
  gof_map = c("nobs", "r.squared")
)

```

- This model still assumes MCAR and reduces variance in the independent variable. 
- Its primary advantage is that it is a quick method to avoid having to throw out cases that have valid data on other important variables.

##  Regression imputation {.smaller}

:::: {.columns}

::: {.column}

- I will predict the value of parental income by other independent variables (but never the dependent variable) using a linear model. 
- In this case, I will transform parental income by the square root as well since it is heavily right skewed. 

::: {.fragment}

```{r}
#| label: model-impute-step1
model <- lm(sqrt(parent_income)~race+pseudo_gpa+
              honor_society+alcohol_use+smoker+
              bandchoir+nsports, 
            data=addhealth)
```

:::

:::

::: {.column .fragment}

Then, I use the `predict` command to get predicted values for all observations and impute the predicted values (or their square, technically) for missing values. 

```{r}
#| label: regimpute-step2

predicted <- predict(model, addhealth)
addhealth$parent_income.regi <- addhealth$parent_income
incmiss <- is.na(addhealth$parent_income)
addhealth$parent_income.regi[incmiss] <- predicted[incmiss]^2
summary(addhealth$parent_income.regi)
```


I still have some missing values, because there were missing values on the variables I used to predict parental income.

:::

::::

##  Random regression imputation

The previous model is deterministic, and will underestimate variance in parental income but I can add a random component to this by sampling from a normal distribution with a mean of zero and standard deviation equal to that of the model residuals. 

```{r}
#| label: random-reg-impute

addhealth$parent_income.rregi <- addhealth$parent_income
addhealth$parent_income.rregi[incmiss] <- (predicted[incmiss]+
                                             rnorm(sum(incmiss), 0, sigma(model)))^2
sd(addhealth$parent_income, na.rm=TRUE)
sd(addhealth$parent_income.regi, na.rm=TRUE)
sd(addhealth$parent_income.rregi, na.rm=TRUE)
```

##  Regression imputation, Add Health

:::: {.columns}

::: {.column}

```{r}
#| echo: false
ggplot(addhealth, aes(x=parent_income.regi, y=nominations,
                      color=parent_income_missing))+
  geom_point(alpha=0.4)+
  geom_smooth(method = "lm", se = FALSE)+
  scale_color_manual(values=c("grey","red"))+
  labs(x="parental income ($1000s)" , 
       y="number of friend nominations received",
       color="Income value imputed",
       title = "Regression Imputation")+
  theme(legend.position = "bottom")
```

:::

::: {.column}

```{r}
#| echo: false
ggplot(addhealth, aes(x=parent_income.rregi, y=nominations, 
                      color=parent_income_missing))+
  geom_point(alpha=0.4)+
  geom_smooth(method = "lm", se = FALSE)+
  scale_color_manual(values=c("grey","red"))+
  labs(x="parental parental income", y="actual or imputed parental income",
       color="Income value imputed",
       title = "Random Regression Imputation")+
  theme(legend.position = "bottom")
```

:::

::::

## Chained equations {.smaller}

As you saw above, predictive imputation still leads to some missing values if the variables used for prediction also contain missing values. We can get around this via an iterative procedure called **chained equations**:

1. All missing values are given some placeholder value. This might be the mean value, for example.
2. For one variable, the placeholder values are removed and missing values put back in. These missing values are then predicted and imputed by some model for this variable.
3. Step 2 is repeated for all variables with missing values. When all variables have been imputed, we have completed one iteration.
4. Steps 2-3 are then repeated again for some number of iterations. The number of iterations necessary may vary by data, but five iterations is typical.

##  Using `mice` to impute a single dataset

The `mice` command will use chained equations to impute missing values on *all* variables when a dataset is fed in. I can then use the `complete` command to extract a full dataset with no missing values. 

```{r}
#| label: chained-equations

library(mice)
imputed <- addhealth |>
  select(nominations, race, gender, grade, pseudo_gpa, honor_society, alcohol_use,
         smoker, bandchoir, nsports, parent_income) |>
  mice(m = 1, print = FALSE)
addhealth_full <- complete(imputed, 1)
apply(is.na(addhealth_full), 2, sum)
```

##  Comparison of methods

```{r}
#| label: tbl-impute-compare
#| tbl-cap: Models predicting friend nominations with different methods for missing values
#| echo: false

addhealth$parent_income.chosen <- addhealth$parent_income
model_delete <- lm(nominations~parent_income.chosen+smoker+alcohol_use+nsports, 
                  data=addhealth)
addhealth$parent_income.chosen <- addhealth$parent_income.meani
model_meani <- lm(nominations~parent_income.chosen+smoker+alcohol_use+nsports, 
                  data=addhealth)
model_meandummy <- update(model_meani, .~.+parent_income_missing)
addhealth$parent_income.chosen <- addhealth$parent_income.randi
model_randi <- lm(nominations~parent_income.chosen+smoker+alcohol_use+nsports, 
                  data=addhealth)
addhealth$parent_income.chosen <- addhealth$parent_income.regi
model_regi <- lm(nominations~parent_income.chosen+smoker+alcohol_use+nsports, 
                 data=addhealth)
addhealth$parent_income.chosen <- addhealth$parent_income.rregi
model_rregi <- lm(nominations~parent_income.chosen+smoker+alcohol_use+nsports, 
                 data=addhealth)
addhealth_full$parent_income.chosen <- addhealth_full$parent_income
model_ce <- lm(nominations~parent_income.chosen+smoker+alcohol_use+nsports, 
                 data=addhealth_full)

modelsummary(
  list(
    "deletion" = model_delete,
    "mean" = model_meani,
    "mean + dummy" = model_meandummy,
    "random" = model_randi,
    "regression" = model_regi,
    "random regression" = model_rregi,
    "chained equations" = model_ce
    ),
  coef_map = cm,
  output = "gt",
  stars = TRUE,
  gof_map = c("nobs", "r.squared")
)
```

##  Imputation variability

- The methods of random imputation have the benefit of preserving the standard deviation of the imputed variable and therefore calculating correct standard errors, but they also introduce a new source of uncertainty. 
- Each time I do an imputation with a random component (e.g. random regression, chained equation), I will get a somewhat different set of values. 
- Therefore, we now have **imputation variability** to add to our inferential concerns alongside **sampling variability**. 
- We can use **multiple imputation** to adjust our results for **imputation variability**.

## Multiple imputation process {.smaller}

1. Use imputation process with random component to impute missing values and repeat this process to produce $m$ separate **complete datasets**. Each of these datasets will be somewhat different due to the randomization of imputation. Usually $m=5$ is sufficient.
2. Run $m$ separate parallel models on each imputed **complete dataset**. As a result, you will have $m$ sets of regression coefficients and standard errors. 
3. Pool the regression coefficients across datasets by taking the mean across all $m$ datasets. 
4. Pool standard errors by taking the mean across all $m$ datasets *plus* the between model standard deviation in coefficients. The formula for the overall standard error is:

::: {.fragment}

$$SE_{\beta}=\sqrt{W+(B+\frac{B}{m})}$$
Where $W$ is the squared mean standard error across all $m$ datasets, and $B$ is the variance in coefficient estimates calculated across all $m$ models.

:::

## Multiple imputation with the `mice` package

To get multiple complete datasets with `mice` we just need to specify the number of distinct complete datasets to create in the second argument:

```{r}
#| echo: false

addhealth <- addhealth |>
  select(grade, race, gender, nominations, alcohol_use, smoker, pseudo_gpa,
         honor_society, bandchoir, nsports, parent_income)
```

```{r}
#| label: mice-imputations

imputations <- mice(addhealth, 5, printFlag=FALSE)
```

- The imputations object now contains five fully complete imputed datasets. Its possible to extract any one of these datasets with the command `complete(imputations, i)` where `i` is replaced by a number between 1 and 5. 
- We can now conduct our parallel analysis on these five datasets and combine results. There is an easy and a hard way to do this. Its useful to know both. 

##  Easy way: let `mice` do the hard work

The `mice` package has a lot of nice features, including an object specific function for the `with` command and a `pool` command that make multiple imputation as easy as falling off a log:

```{r easyway_mi}
#| label: easyway-mi
model_mi <- pool(with(imputations, 
                      lm(nominations~parent_income+smoker+alcohol_use+nsports)))
summary(model_mi)
```

##  Hard way: for-loop {.smaller}

The hard way isn't really that hard. It is useful to know for cases where the easy way won't work, such as when you need to run complicated models (using `svydesign` would be an example).

```{r}
#| label: forloop-mi
b <- se <- NULL
for(i in 1:5) {
  imputation <- complete(imputations, i)
  model <- lm(nominations~parent_income+smoker+alcohol_use+nsports, 
              data=imputation)
  b <- cbind(b, coef(model))
  se <- cbind(se, summary(model)$coef[,2])
}
b
```

The `b` and `se` objects are matrices that contain the coefficients and standard errors, respectively, for each model on the column.

##  Hard way: pool the results

Now we can pool the results using the `b` and `se` matrices and some creative use of `apply` commands. 

```{r}
#| label: pool-forloop-mi

b.pool <- apply(b,1,mean)
between.var <- apply(b,1,var)
within.var <- apply(se^2,1,mean)
se.pool <- sqrt(within.var+between.var+between.var/5)
t.pool <- b.pool/se.pool
pvalue.pool <- (1-pnorm(abs(t.pool)))*2
tibble(b.pool, se.pool, t.pool, pvalue.pool) |>
  gt()
```

