---
title: "Model Complications"
execute:
  error: true
---

```{r}
#| label: setup
#| include: false
library(here)
source(here("scripts","check_packages.R"))
source(here("scripts","set_theme.R"))
source(here("scripts","load_example_data.R"))
source(here("scripts", "preamble.R"))
```

# The Linear Model Revisited {.center background-color="black" background-image="images/jeremy-bishop-FzrlPh20l7Q-unsplash.jpg"}

## Review of linear model {.smaller}

$$\hat{y}_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}$$

- $\hat{y}_i$ is the predicted value of the outcome/dependent variable which must be quantitative.
- The $x$ values are different independent/explanatory variables which may be quantitative or categorical (by using dummy coding).
- The $\beta$ values are the slopes/effects/coefficients that, *holding all other independent variables constant*, tell us the:
    - The predicted change in $y$ for a one unit increase in $x$ if $x$ is quantitative
    - The mean difference in $y$ between the indicated category and the reference category if $x$ is categorical.
    
. . . 

### Interpret these values

```{r}
#| label: lm-example
coef(lm(nominations~nsports+I(parent_income-45)+gender, data=popularity))
```

## The residual term {.smaller}

$$\epsilon_i=y_i-\hat{y}_i$$

The residual/error term $\epsilon_i$ gives us the difference between the actual value of the outcome variable for a given observation and the value predicted by the model.

. . . 

Lets use algebra to rewrite this equation with $y_i$ on the left-hand side:

$$y_i=\hat{y}_i+\epsilon_i$$

. . . 

If we plug in our linear model formula for $\hat{y}_i$, we can get the full model formula:

$$y_i=\underbrace{\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}}_{\hat{y}_i}+\epsilon_i$$

## Linear model as a partition of the variance in $y$ {.smaller}

$$y_i = \underbrace{\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\ldots+\beta_px_{ip}}_\text{structural} +
    \underbrace{\epsilon_i}_\text{stochastic}$$
    
- The structural part is the predicted value from our model which is typically a linear function of the independent variables.
- The stochastic component is the leftover residual or error component, that is not accounted for by the model.

. . . 

Depending on disciplinary norms, there are different conceptual ways to view this basic relationship:

- **Description:** observed = summary + residual
- **Prediction:** observed = predicted + error
- **Causation:** observed = true process + disturbance

## Calculating marginal effects {.smaller}

The **marginal effect** of $x$ on $y$ is simply the predicted change in $y$ for a one unit increase in $x$ from its current value, holding all else constant.

. . . 

In a basic linear model, the marginal effect is just given by the slope itself. 

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\epsilon_i$$

- $\beta_1$ is the marginal effect of $x_1$
- $\beta_2$ is the marginal effect of $x_2$

. . . 

Technically, the marginal effect is the derivative of $y$ with respect to a given $x$. This gives us the **tangent line** for the curve at any value of $x$.

$$\frac{\partial y}{\partial x_1}=\beta_1$$
$$\frac{\partial y}{\partial x_2}=\beta_2$$

`r emo::ji("scream")` I am not expecting you to know the calculus behind this, but it may help some people.

## Marginal effects with interaction terms {.smaller}

:::: {.columns}

::: {.column .fragment}

Marginal effects can get more complicated when we move to more complex model. Consider this model with an interaction term:

$$y_i=\beta_0+\beta_1x_{i1}+\beta_2x_{i2}+\beta_3(x_{i1})(x_{i2})+\epsilon_i$$

::: {.fragment}

The marginal effects are now given by:

$$\frac{\partial y}{\partial x_1}=\beta_1+\beta_3x_{i2}$$
$$\frac{\partial y}{\partial x_2}=\beta_2+\beta_3x_{i1}$$

This is a very math-y way of saying: **the main effects depend on the effect of the other variable**.

:::

:::

::: {.column .fragment}

```{r marginal-fx}
model <- lm(nominations~nsports*gender, data=popularity)
as.data.frame(coef(model))
```

::: {.fragment}

The marginal effect for number of sports played is:

$$0.49-0.09(male_i)$$

The marginal effect for gender is:

$$-0.54-0.09(nsports_i)$$

:::

:::

::::

## Marginal effects plot {.smaller}

:::: {.columns}

::: {.column}

```{r}
#| label: fig-marginal-fx-plot-code
#| eval: false
nsports <- 0:6
gender_diff <- -0.54-0.09*nsports
ggplot(tibble(nsports, gender_diff),
       aes(x=nsports, y=gender_diff))+
  geom_line()+
  labs(x="number of sports played",
       y="predicted popularity difference between boys and girls")
```

:::

::: {.column}

```{r}
#| label: fig-marginal-fx-plot-exec
#| fig-cap: Marginal effect of gender difference on popularity by sports played
#| echo: false
nsports <- 0:6
gender_diff <- -0.54-0.09*nsports
ggplot(data.frame(nsports, gender_diff),
       aes(x=nsports, y=gender_diff))+
  geom_line()+
  labs(x="number of sports played",
       y="predicted popularity difference between boys and girls")
```

:::

::::

## Two key assumptions of linear models {.smaller}

:::: {.columns}

::: {.column .fragment}

### Linearity
```{r linear-violation, echo=FALSE, fig.height=4}
ggplot(subset(gapminder, year==2007), aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  geom_text_repel(data=subset(gapminder, 
                              year==2007 & gdpPercap>5000 & lifeExp<60), 
                  aes(label=country), size=2)+
  labs(x="GDP per capita", y="Life expectancy at birth", subtitle = "2007 data from Gapminder")+
  scale_x_continuous(labels=scales::dollar)
```

* If you fit a model with the wrong functional form, it is considered a *specification error*.
* We can correct this through a variety of more advanced model specifications.


:::

::: {.column .fragment}

### Error terms are iid
```{r heteroskedasticity, echo=FALSE, fig.height=4}
model <- lm(box_office~metascore, data=movies)
ggplot(augment(model), aes(x=.fitted, y=.resid))+
  geom_jitter(alpha=0.2)+
  geom_hline(yintercept = 0, linetype=2)+
  labs(x="predicted values of box office returns by tomato rating", y="model residuals")
```
* iid = **independent and identically distributed** which is typically violated either by **heteroscedasticity** or **autocorrelation**. 
* The consequence of violating the i.i.d. assumption is usually incorrect standard errors. 
:::

::::

## How are linear model parameters estimated? {.smaller}

`r emo::ji("warning")` Heavy math ahead!

- We have simple formulas for the slope and intercept for a bivariat model. 
- With multiple independent variables, a simple formula will not suffice. To estimate model parameters with multiple independent variables we need to use some matrix algebra. 

. . . 

### The matrix algebra approach to linear models {.smaller}

We can use matrix algebra to represent our linear regression model equation using one-dimensional **vectors** and two-dimensional **matrices**. 

$$\mathbf{y}=\mathbf{X\beta+\epsilon}$$

- $\mathbf{y}$ is a vector of known values of the independent variable of length $n$.
- $\mathbf{X}$ is a matrix of known values of the independent variables of dimensions $n$ by $p+1$.
- $\mathbf{\beta}$ is a vector of to-be-estimated values of intercepts and slopes of length $p+1$.
- $\mathbf{\epsilon}$ is a vector of residuals of length $n$ that will be equal to $\mathbf{y-X\beta}$.

## `r emo::ji("warning")` Linear model in matrix form {.smaller}

::: {.columns}

::: {.column}

### Known Quantities

$$\mathbf{y}=\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{pmatrix}$$



$$\mathbf{X}=\begin{pmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1p}\\
1 & x_{21} & x_{22} & \ldots & x_{2p}\\
\vdots & \vdots & \ldots & \vdots\\
1 & x_{n1} & x_{n2} & \ldots & x_{np}\\
\end{pmatrix}$$

:::

::: {.column}

### Need to Estimate

$$\mathbf{\beta}=\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{p}\\
\end{pmatrix}$$

$$\mathbf{\epsilon}=\begin{pmatrix}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{n}\\
\end{pmatrix}$$

:::

::::

## `r emo::ji("warning")` Linear model in matrix form 

Putting it all together gives us this beast `r emo::ji("japanese_ogre")`:
$$\begin{pmatrix}
y_{1}\\
y_{2}\\
\vdots\\
y_{n}\\
\end{pmatrix}=\begin{pmatrix}
1 & x_{11} & x_{12} & \ldots & x_{1p}\\
1 & x_{21} & x_{22} & \ldots & x_{2p}\\
\vdots & \vdots & \ldots & \vdots\\
1 & x_{n1} & x_{n2} & \ldots & x_{np}\\
\end{pmatrix}\begin{pmatrix}
\beta_{0}\\
\beta_{1}\\
\vdots\\
\beta_{p}\\
\end{pmatrix}+\begin{pmatrix}
\epsilon_{1}\\
\epsilon_{2}\\
\vdots\\
\epsilon_{n}\\
\end{pmatrix}$$

## `r emo::ji("warning")` Minimize sum of squared residuals {.smaller}

Our goal is to choose a $\mathbf{\beta}$ vector that minimizes the sum of squared residuals, $SSR$, which is just given by the $\epsilon$ vector squared and summed up. We can rewrite the matrix algebra formula to isolate $e$ on one side:

$$\begin{align*}
\mathbf{y}&=&\mathbf{X\beta+\epsilon}\\
\epsilon&=&\mathbf{y}-\mathbf{X\beta}
\end{align*}$$

. . . 

In matrix form, $SSR$ can be represented as a function of $\mathbf{\beta}$, like so:

$$\begin{align*}
SSR(\beta)&=&(\mathbf{y}-\mathbf{X\beta})'(\mathbf{y}-\mathbf{X\beta})\\
&=&\mathbf{y}'\mathbf{y}-2\mathbf{y}'\mathbf{X\beta}+\mathbf{\beta}'\mathbf{X'X\beta}
\end{align*}$$

## `r emo::ji("warning")` Minimize sum of squared residuals {.smaller}

$$\begin{align*}
SSR(\beta)&=&(\mathbf{y}-\mathbf{X\beta})'(\mathbf{y}-\mathbf{X\beta})\\
&=&\mathbf{y}'\mathbf{y}-2\mathbf{y}'\mathbf{X\beta}+\mathbf{\beta}'\mathbf{X'X\beta}
\end{align*}$$

We want to choose a $\mathbf{\beta}$ to plug into this function that provides the smallest possible value (the minimum). 

. . . 

It turns out that we can get this value by using calculus to get the derivative with respect to $\mathbf{\beta}$ and solving for zero:

$$0=-2\mathbf{X'y}+2\mathbf{X'X\beta}$$

. . . 

Applying some matrix algebra will give us the estimator of $\mathbf{\beta}$:

$$\mathbf{\beta}=(\mathbf{X'X})^{-1}\mathbf{X'y}$$

## Estimating linear models manually

```{r matrixreg}
#set up the design matrix
X <- as.matrix(cbind(rep(1, nrow(movies)), movies[,c("runtime","box_office")]))
#the outcome variable vector
y <- movies$metascore
#crossprod(X) will do matrix multiplication, solve will invert
beta <- solve(crossprod(X))%*%crossprod(X,y)
beta
```

. . . 

```{r}
#how does it compare to lm?
model <- lm(metascore~runtime+box_office, data=movies)
coef(model)
```


## `r emo::ji("warning")` Estimating standard errors {.smaller}

If we treat $\sigma^2$ as the variance of the error term $\epsilon$, then we can also use matrix algebra to calculate the **covariance matrix**:

$$\sigma^{2}(\mathbf{X'X})^{-1}$$

The values of this matrix give us information about the correlation between different independent variables. Most importantly, the square root of the diagonal values of this matrix are the standard errors for the estimated values of $\beta$. 

In practice, we don't have $\sigma^2$, but we can estimate from the fitted values of $y$ by:

$$s^2=\frac{\sum(y_i-\hat{y}_i)^2}{n-p-1}$$
We can then use these estimated standard errors to calculate t-statistics and p-values, confidence intervals, and so on. 

## Calculating SEs manually {.smaller}

```{r matrixse}
y.hat <- X%*%beta
df <- length(y)-ncol(X)
s.sq <- sum((y-y.hat)^2)/df
covar.matrix <- s.sq*solve(crossprod(X))
se <- sqrt(diag(covar.matrix))
t.stat <- beta/se
p.value <- 2*pt(-1*abs(t.stat), df)
data.frame(beta, se, t.stat, p.value)
summary(model)$coef
```


# Modeling Non-Linearity {.center background-color="black" background-image="images/fabian-quintero-nq02weVF_mk-unsplash.jpg"}

## Linear models fit straight lines

```{r}
#| label: fig-life-exp-non-linear
#| fig-cap: The relationship between life expectancy and GDP per capita is clearly not a straight line!
#| echo: false
#| fig-width: 24
#| fig-height: 14

gapminder |>
  filter(year == 2007) |>
  ggplot(aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  geom_text_repel(data=subset(gapminder, year==2007 & gdpPercap>5000 & lifeExp<60), 
                  aes(label=country), size=2)+
  labs(x="GDP per capita", y="Life expectancy at birth", subtitle = "2007 data from Gapminder")+
  scale_x_continuous(labels=scales::dollar)
```

::: {.notes}

- If you try to model a relationship using a straight line when that relationship is more complex (i.e. "bendy") then your model will not fit well and will not represent the relationship accurately. 
- The technical way of saying this is that the functional form of the relationship is mis-specified in the model. 
- The most common cases of non-linearity are **diminishing returns** and **exponential** relationships, although more complex forms of non-linearity are also possible. 
- The example to the left shows a clear diminishing returns between life expectancy and GDP per capita.

:::

## Non-linearity can be hard to detect

```{r}
#| label: fig-non-linear-movies
#| fig-cap: Is this relationship non-linear?
#| echo: false
#| fig-width: 24
#| fig-height: 14

ggplot(movies, aes(x=rating_imdb, y=box_office))+
  geom_jitter(alpha=0.2)+
  scale_y_continuous(labels = scales::dollar)+
  labs(x="IMDB rating", y="Box Office Returns (millions USD)")
```

## Two techniques for detecting non-linearity

::: {.columns}

::: {.column .fragment}

### Non-linear smoothing

```{r}
#| echo: false

gapminder |>
  filter(year == 2007) |>
  ggplot(aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(se=FALSE)+
  geom_text_repel(data=subset(gapminder, year==2007 & gdpPercap>5000 & lifeExp<60), 
                  aes(label=country), size=2)+
  labs(x="GDP per capita", y="Life expectancy at birth", subtitle = "2007 data from Gapminder")+
  scale_x_continuous(labels=scales::dollar)
```

:::

::: {.column .fragment}

### Diagnostic residual plot

```{r}
#| echo: false

model <- lm(lifeExp~gdpPercap, data=subset(gapminder, year==2007))
ggplot(augment(model), aes(x=.fitted, y=.resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_smooth(se=FALSE)+
  labs(x="fitted values of life expectancy", y="model residuals")+
  theme_bw()
```

:::

::::

## Non-linear smoothing

- A smoothing function uses the values of the $y$ for the closest neighbors for a given observation of $x$ to calculate a smoothed value of $y$ that will reduce extreme values.
- Smoothing functions vary by 
     - What function is used to calculate the smoothed values (e.g. mean, median)
     - The span of how many neighbors are considered. Larger spans will lead to smoother lines.

## Median smoothing box office returns for *Rush Hour 3*

:::: {.columns}

::: {.column width = "30%"}

![Rush Hour 3 Poster](images/rushhour3.jpg)

:::

::: {.column width = "70%"}

```{r}
#| label: tbl-smooth-rush-hour-3
#| tbl-cap: Calculating smoothed box office returns for Rush Hour 3 with two neighbors either way.
#| echo: false

movies <- movies |> arrange(rating_imdb)
i <- which(movies$title=="Rush Hour 3")

temp <- movies |>
  slice((i-2):(i+2)) |>
  select(title, rating_imdb, box_office)
temp$box_office_smooth <- c(NA, NA, median(temp$box_office), NA, NA)
temp |>
  gt() |>
  sub_missing(missing_text = "-") |>
  fmt_currency(starts_with("box_office"), decimals = 1) |>
  cols_label(title = "Title",
             rating_imdb = "IMDB Rating",
             box_office = "Box Office (millions)",
             box_office_smooth = "Smoothed Box Office (median)")

```

:::

::::

## Applying a median smoother

```{r}
#| label: fig-median-smoothing
#| fig-cap: Different median smoothers over scatterplot movie box office returns by IMDB rating
#| echo: false
#| fig-width: 24
#| fig-height: 14

smoothed <- movies |> 
  select(rating_imdb, box_office) |>
  mutate(box_office_smooth5 = runmed(box_office, 5),
         box_office_smooth501 = runmed(box_office, 501)) |>
  select(-box_office) |>
  pivot_longer(cols = starts_with("box_office_"), 
               names_prefix = "box_office_", 
               names_to = "smoothing", 
               values_to = "box_office") |>
  mutate(smoothing = factor(smoothing, 
                            levels = c("smooth5", "smooth501"),
                            labels = c("2 neighbors each way", 
                                       "250 neighbors each way")))

ggplot(movies, aes(x = rating_imdb, y = box_office))+
  geom_point(alpha = 0.2)+
  geom_point(data=subset(movies, title=="Rush Hour 3"), color = "red")+
  geom_line(data = smoothed, aes(color = smoothing), linewidth = 1.5)+
  scale_color_manual(values = wes_palette("Moonrise2"))+
  geom_label_repel(data=subset(movies, title=="Rush Hour 3"),
                   aes(label=title), min.segment.length = 0)+
  labs(x="IMDB rating", y="Box Office Returns (millions)")
```

## The LOESS smoother {.smaller}

:::: {.columns}

::: {.column}

### LOcally Estimated Scatterplot Smoothing

- Each smoothed value is determined from a model that includes:
  - polynomial terms (more on that later)
  - weighting of observations by distance from focal point
- LOESS does very nice smoothing but is computationally expensive
- Because of the weighting, LOESS can and does take a very large span of data for each focal point

:::

::: {.column}

```{r}
#| label: fig-loess
#| fig-cap: Use LOESS smoothing for nice smooth lines
#| code-line-numbers: "3"
ggplot(movies, aes(x=rating_imdb, y=box_office))+
  geom_jitter(alpha=0.2)+
  geom_smooth(method="loess", se=FALSE)+
  labs(x="IMDB Rating", 
       y="Box Office Returns (millions USD)")
```

:::

::::

## Adjusting span {{< fa user-ninja >}} {.smaller}

:::: {.columns}

::: {.column}

```{r}
#| label: adjust-span-code
#| eval: false
#| code-line-numbers: "4-9"

ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="loess", span=1, 
              se=FALSE, color="green")+
  geom_smooth(method="loess", span=0.75, 
              se=FALSE, color="red")+
  geom_smooth(method="loess", span=0.25, 
              se=FALSE, color="blue")+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")+
  scale_x_continuous(labels=scales::dollar)
```

The default `span` is 0.75 which is 75% of observations.

:::

::: {.column}

```{r}
#| label: adjust-span-exec
#| echo: false
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="loess", span=1, 
              se=FALSE, color="green")+
  geom_smooth(method="loess", span=0.75, 
              se=FALSE, color="red")+
  geom_smooth(method="loess", span=0.25, 
              se=FALSE, color="blue")+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")+
  scale_x_continuous(labels=scales::dollar)
```

:::

::::

## `r emo::ji("warning")` LOESS with large `n`  will `r emo::ji("death")` your `r emo::ji("computer")` {.smaller}

:::: {.columns}

::: {.column}

### Use a GAM instead {.smaller}

Generalized Additive Models (GAM) are another way to create non-linear smoothing that is less computational intensive that LOESS but with similar results.

```{r}
#| label: gam-example-code
#| eval: false
#| code-line-numbers: "6-7"
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="loess",
              se=FALSE, color="blue")+
  geom_smooth(method="gam",
              se=FALSE, color="red")+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")+
  scale_x_continuous(labels=scales::dollar)
```

:::

::: {.column}

```{r}
#| label: gam-example-exec
#| echo: false
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="loess",
              se=FALSE, color="blue")+
  geom_smooth(method="gam",
              se=FALSE, color="red")+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")+
  scale_x_continuous(labels=scales::dollar)
```

:::

::::

## Or let `geom_smooth` figure out best smoother {.smaller}

:::: {.columns}


::: {.column}

```{r}
#| label: geom-smooth-code
#| eval: false
ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width=1)+
  geom_smooth(method="lm", se=FALSE, 
              color="blue", linewidth = 1.5)+
  geom_smooth(se=FALSE, color="red", linewidth = 1.5)+
  scale_y_continuous(labels = scales::dollar)+
  labs(x="age", y="hourly wages")
```

- For datasets over 1000 observations, `geom_smooth` will use GAM and otherwise defaults to LOESS. 
- Don't forget you can also specify a linear fit with `method="lm"`.

:::

::: {.column}

```{r}
#| label: geom-smooth-exec
#| echo: false
ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width=1)+
  geom_smooth(method="lm", se=FALSE, 
              color="blue")+
  geom_smooth(se=FALSE, color="red")+
  scale_y_continuous(labels = scales::dollar)+
  labs(x="age", y="hourly wages")
```

:::

::::

## Residual plots

:::: {.columns}

::: {.column}

- A scatterplot of the residuals vs. the fitted values from a model can also be useful for detecting non-linearity.
- If the relationship is linear, then we should expect to see no sign of a relationship in this plot. Drawing a smoothed line can be useful for this diagnosis.
- Residual plots can also help to detect **heteroskedasticity** which we will talk about later.

:::

::: {.column}

```{r}
#| label: fig-resid-plot-le
#| fig-cap: Residual plots can be used for multiple diagnostic purposes
#| echo: false

model <- lm(lifeExp~gdpPercap, data=subset(gapminder, year==2007))
ggplot(augment(model), aes(x=.fitted, y=.resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_smooth(se=FALSE)+
  labs(x="fitted values of life expectancy", y="model residuals")
```

:::

::::

## Using `broom` and `augment` to get model data

```{r}
#| label: augment-broom

library(broom)
model <- lm(lifeExp~gdpPercap, data=subset(gapminder, year==2007))
augment(model)
```

## Creating a residual plot

:::: {.columns}

::: {.column}

```{r}
#| eval: false

ggplot(augment(model), 
       aes(x=.fitted, y=.resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_smooth(se=FALSE)+
  labs(x="fitted values of life expectancy", 
       y="model residuals")
```

:::

:::{.column}

```{r}
#| echo: false

ggplot(augment(model), 
       aes(x=.fitted, y=.resid))+
  geom_point()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_smooth(se=FALSE)+
  labs(x="fitted values of life expectancy", 
       y="model residuals")
```

:::

::::

## Its non-linear, so now what?

:::: {.columns}

::: {.column .frament}

### Transform variables
```{r}
#| echo: false
#| fig-height: 4
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  scale_x_log10(labels=scales::dollar)+
  labs(x="GDP per capita (log scale)", 
       y="Life expectancy at birth")
```

:::

::: {.column .fragment}

### Polynomial terms
```{r}
#| echo: false
#| fig-height: 4
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", 
              formula=y~x+I(x^2)+I(x^3),
              se=FALSE)+
  scale_x_continuous(labels=scales::dollar)+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")
```

:::

::: {.column .fragment}

### Create splines
```{r}
#| echo: false
#| fig-height: 4
temp <- subset(gapminder, year==2007)
cutoff <- 7500
temp$gdp.spline <- ifelse(temp$gdpPercap<cutoff, 0,
                          temp$gdpPercap-cutoff)
model <- lm(lifeExp~gdpPercap+gdp.spline, data=temp)
predict_df <- data.frame(gdpPercap=seq(from=0,to=50000,by=100))
predict_df$gdp.spline <- ifelse(predict_df$gdpPercap<cutoff, 0,
                                predict_df$gdpPercap-cutoff)
predict_df$lifeExp <- predict(model, newdata=predict_df)
ggplot(temp, 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_line(data=predict_df, color="blue", size=1)+
  scale_x_continuous(labels=scales::dollar)+
  labs(x="GDP per capita", 
       y="Life expectancy at birth")
```

:::

::::

## Transforming variables {.smaller}

:::: {.columns}

::: {.column width = "30%"}

![](images/werewolf-transformation.jpg)

:::

::: {.column width = "70%"}

A transformation is a mathematical function that changes the value of a quantitative variable. There are many transformations that one could apply, but we will focus on one - the **log** transformation. This is the most common transformation used in the social sciences. 

Transformations are popular because they can solve multiple problems:

- A transformation can make a non-linear relationship look linear. 
- A transformation can make a skewed distribution more symmetric. 
- A transformation can reduce the impact of extreme outliers. 

:::

::::

## Plotting the log transformation

:::: {.columns}

::: {.column}

```{r}
#| eval: false
#| code-line-numbers: "5"
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  scale_x_log10(labels=scales::dollar)+#<<
  labs(x="GDP per capita (log scale)", 
       y="Life expectancy at birth")
```

- The scale of the independent variable is now multiplicative
- The relationship looks more linear now, but what does it mean?

:::

::: {.column}

```{r}
#| echo: false
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point(alpha=0.7)+
  geom_smooth(method="lm", se=FALSE)+
  scale_x_log10(labels=scales::dollar)+#<<
  labs(x="GDP per capita (log scale)", 
       y="Life expectancy at birth")
```

:::

::::

## The natural log {.smaller}
:::: {.columns}

::: {.column width = "30%"}


![](images/logs.png)

:::

::: {.column width = "70%"}

Although `ggplot` uses log with a base 10, we usually use the *natural log* transformation in practice. Both transformations have the same effect on the relationship, but the natural log provides results that are easier to interpret. 

::: {.fragment}

In R, it is easy to take the natural log of a number by just using the `log` command. Any positive number can be logged. 

```{r}
log(5)
```

The natural log of 5 is 1.609, but what does this mean? 

:::

::: {.fragment}

The natural log of any number is the power that you would have to raise the constant $e$ to in order to get the original number back. In R, we can calculate $e^x$ with `exp(x)`:

```{r}
exp(log(5))
```

:::

:::

::::

## logs make multiplicative relationships additive

:::: {.columns}

::: {.column width = "30%"}

![](images/lincoln_logs.jpg)

:::

::: {.column width = "70%"}

The key feature of the log transformation is that it makes multiplicative relationships additive.

$$log(x*y)=log(x)+log(y)$$

```{r}
log(5*4)
log(5)+log(4)
```

We can use this feature to model relative (percent) change rather than absolute change in our models.

:::

::::

## Logging box office returns gives us a linear fit

```{r}
#| label: fig-box-office-log
#| fig-cap: Note the logarithmic scale on the y-axis
#| echo: false
#| fig-width: 24
#| fig-height: 14

ggplot(movies, aes(x=rating_imdb, y=box_office))+
  geom_jitter(alpha=0.2)+
  geom_smooth(method="lm", color="red", se=FALSE)+
  scale_y_log10(labels = scales::dollar)+
  labs(x="IMDB Rating", y="Box Office Returns (millions)")
```

## What does it mean for the model?

To fit the model, we can just use the log transformation directly in the formula of our `lm` command:

```{r}
model <- lm(log(box_office)~rating_imdb, data=movies)
coef(model)
```

$$\log(\hat{returns}_i)=0.016+0.371(rating_i)$$

How do we interpret the slope?

::: {.fragment}

> A one point increase in IMDB rating is associated with a 0.371 increase in ... the log of box office returns? `r emo::ji("confused")`

:::

## Converting to the original scale {.smaller}

To get back to the original scale of box office returns for the dependent variable, we need to exponentiate both side side of the regression equation by $e$:

$$e^{\log(\hat{returns}_i)}=e^{0.016+0.371(rating_i)}$$

::: {.fragment}

On the left hand side, I will get back predicted box office returns by the definition of logs. On the right hand side, I can apply some of the mathematical properties of logarithms and powers. 

$$\hat{returns}_i=(e^{0.016})(e^{0.371})^{rating_i}$$

```{r}
exp(0.016)
exp(0.371)
```

:::

::: {.fragment}

I now have:
$$\hat{returns}_i=(1.016)(1.449)^{rating_i}$$

:::

## A multiplicative relationship {.smaller}

$$\hat{returns}_i=(1.016)(1.449)^{rating_i}$$

Lets calculate predicted box office returns for IMDB Ratings of 0, 1, and 2. 

::: {.fragment}

$\hat{returns}_i=(1.016)(1.449)^{0}=1.016$

:::

::: {.fragment}

$\hat{returns}_i=(1.016)(1.449)^{1}=1.449(1.449)$

:::


::: {.fragment}

$\hat{returns}_i=(1.016)(1.449)^{2}=1.449(1.449)(1.449)$

:::

- For each one unit increase in the independent variable, you multiply the previous predicted value by 1.449 to get the new predicted value. Therefore the predicted value increases by 44.9%.
- The model predicts that movies with a zero IMDB rating make 1.02 million dollars, on average. Every one point increase in the IMDB rating is associated with a 44.9% increase in box office returns, on average.

## General form and interpretation

$$\log(\hat{y}_i)=b_0+b_1(x_i)$$

- You must apply the `exp` command to your intercept and slopes in order to interpret them. 
- The model predicts that the mean of $y$ when $x$ is zero will be $e^{b_0}$. 
- The model predicts that each one unit increase in $x$ is associated with a multiplicative change of $e^{b_1}$ in $y$. It is often easiest to express this in percentage terms. 
- An **absolute** change in $x$ is associated with a **relative** change in $y$.

## Interpret these numbers {.smaller}

```{r}
coef(lm(log(box_office)~relevel(maturity_rating, "PG"), data=movies))
```

$$\log(\hat{returns}_i)=3.22+0.50(G_i)-0.24(PG13_i)-1.93(R_i)$$

- $e^{3.28}=26.58$: PG-rated movies make 26.58 million dollars on average.
- $e^{0.75}=2.12$: G-rated movies make 112% more than PG-rated movies, on average.
- $e^{-0.27}=0.76$: PG-13 rated movies make 76% as much as (or 24% less than) PG-rated movies, on average. 
- $e^{-1.79}=0.17$: R-rated movies make 17% as much as (or 83% less than) PG-rated movies, on average.

## Approximating small effects {.smaller}

When $x$ is small (say $x<0.2$), then $e^x\approx1+x$.

We can use this fact to roughly approximate coefficients/slopes as percent change when they are small.

```{r}
model <- lm(log(box_office)~runtime, data=movies)
coef(model)
exp(coef(model))
```

- If we do the full exponentiating, we can see that a one minute increase in runtime is associated with a 3.9% increase in box office returns.
- The actual percentage increase is very close to what we got for the slope of the non-exponentiated slope (0.038). So, you can often get a ballpark estimate without having to exponentiate.

## Logging the independent variable {.smaller}

:::: {.columns}

::: {.column width = "30%"}

Lets return to the relationship between GDP per capita and life expectancy that fit well as a linear relationship when we logged GDP per capita. Lets run the model:

```{r log-gdp-model}
model <- lm(lifeExp~log(gdpPercap), 
            data=subset(gapminder, 
                        year==2007))
round(coef(model), 5)
```

:::

::: {.column .fragment width = "70%"}

How do we interpret these results? This case requires something different than the case where we logged the dependent variable.

Our basic model for life expectancy by GDP per capita is:

$$\hat{y}_i=4.9+7.2\log{x_i}$$
What is the predicted value of life expectancy at $1 GDP?

$$\hat{y}_i=4.9+7.2\log{1} = 4.9+7.2 * 0=4.9$$

What happens when we increase GDP per capita by 1% (from 1 to 1.01)?

$$\hat{y}_i=4.9+7.2\log{1.01} = 4.9+7.2 * 0.01=4.9+0.072$$

Predicted life expectancy increases by 0.072 years. 

:::

::::

## The 1% increase is the same at all $x$

How much does $y$ increase when $x$ goes up by one percent?

$$(4.9+7.2\log{1.01x})-(4.9+7.2\log{x})$$
$$7.2\log{1.01x}-7.2\log{x}$$
$$7.2(\log{1.01}+\log{x})-7.2\log{x}$$

$$7.2\log{1.01}+7.2\log{x}-7.2\log{x}$$
$$7.2\log{1.01} \approx 7.2 (0.01) = 0.072$$




## General form and interpretation

$$\hat{y}_i=b_0+b_1\log(x_i)$$

- A one percent increase in $x$ is associated with a $b_1/100$ unit change in $y$, on average.
- A **relative** change in $x$ is associated with an **absolute** change in $y$.
- $exp(b_0)$ gives the predicted value of $y$ when $x$ equals one. 
- Keep in mind that the $log(0)$ is negative infinity so you cannot predict the value of $y$ when $x=0$. 

## Logging both variables

```{r}
#| label: fig-log-both
#| fig-cap: Logging both variables can sometimes solve multiple problems
#| echo: false
#| fig-width: 24
#| fig-height: 14

ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.05, width=0.1)+
  geom_smooth(se=FALSE, method="lm", linewidth = 1.5)+
  scale_y_log10(labels = scales::dollar)+
  scale_x_log10()+
  labs(x="age (log-scale)", y="hourly wages (log-scale)")
```

## The elasticity model

```{r}
#| label: elasticity-model

model <- lm(log(wages)~log(age), data=earnings)
coef(model)
```

- This is actually the easiest model to interpret. We can interpret the slope directly as the percent change in $y$ for a one percent increase in $x$. 
- Calculating the percent change in one variable by percent change in another is what economists call an **elasticity** so this model is often called an **elasticity** model.
- The model predicts that a one percent increase in age is associated with a 0.51% increase in wages, on average. 

## A cheat sheet

| Which variable logged | Non-linear shape   | Change in x | Change in y | Interpret $\beta_1$ |
|-----------------------|--------------------|-------------|-------------|---------------------|
| Independent variable  | diminishing returns| relative    | absolute    | $\beta_1/100$       |
| Dependent variable    | exponential        | absolute    | relative    | $e^{\beta_1}$       |
| Both variables        | both types         | relative    | relative    | $\beta_1$           |

---

## When $x<=0$, logging is bad {.smaller}

:::: {.columns}

::: {.column}

### Log problems

$$log(0)=-\infty$$
$$log(x)\text{ is undefined when }x<0$$

### Try the square/cube root

- The square root transformation has a similar effect to the log transformation but can include zero values. 
- The cube root transformation can also include negative values. 
- The downside of square/cube root transformations is that values are not easy to interpret. 

:::

::: {.column}

```{r}
#| label: sqrt-transform
#| echo: false
ggplot(movies, aes(x=awards, y=box_office))+
  geom_vline(xintercept = 0, linetype=2)+
  geom_jitter(alpha=0.2)+
  scale_y_log10()+
  scale_x_sqrt()+
  geom_smooth(se=FALSE)+
  labs(x="Number of Oscars (square root scale)",
       y="Box Office Returns (log scale)")
```

:::

::::

## Polynomial models

.pull-left[
### Polynomial expression

A **polynomial** expression is one that adds together terms involving multiple powers of a single variable. 

if we include a squared value of $x$ we get the classic formula for a parabola:

$$y=a+bx+cx^2$$

```{r echo=FALSE, fig.height=4}
x <- 0:100
y <- 10+5*x-0.04*x^2
plot(x,y, type="l", lwd=2, las=1)
text(40, 50, expression(y == 10 + 5*x + 0.04*x^2))
```

]

--

.pull-right[
### A polynomial model

We can fit such a parabola in a linear model by including a new variable that is simply the square of the original variable:

$$\hat{y}_i=\beta_0+\beta_1x_i+\beta_2x_i^2$$

```{r quadratic-model}
model <- lm(wages~I(age-40)+I((age-40)^2), 
            data=earnings)
coef(model)
```

Our model is:

$$\hat{y}_i=26.9+0.3171(x_i-40)-0.0178(x_i-40)^2$$

How do we interpret the results?
]

---

## Calculate the marginal effect

With polynomial terms, the marginal effect of $x$ is more complicated because our $x$ shows up in more than one place in the equation:

$$y_i=\beta_0+\beta_1x_i+\beta_2x_i^2+\epsilon_i$$

--

By calculus, we can mathemagically get the marginal effect for this model as:

$$\frac{\partial y}{\partial x}=\beta_1+2*\beta_2x$$

--

So, for our case:

$$\hat{y}_i=26.9+0.3171(x_i-40)-0.0178(x_i-40)^2$$

the marginal effect of age on wages is given by:

$$0.3171+2*-0.0178(x-40)=0.3171-0.0356(x-40)$$

- At age 40 (the zero value), a one year increase in age is associated with a salary increase of $0.32, on average. 
- For every year over 40, this increase is smaller by $0.0356. For every age younger than 40, this increase is larger by $0.0356.

---

## Finding the inflection point

.pull-left[
If the effect of age on wages goes down by $0.0356 for every year over 40, then at some point the positive effect of age on wages will become negative. 

We can figure out the value of age at this **inflection point** by setting the effect to zero and solving for x. In general, this will give us:

$$\beta_1/(-2*\beta_2)$$

In our case, we get: 

$$0.3171/(-2*-0.0178)=0.3171/0.0356=8.91$$

So the model predicts that the effect of age on wages will shift from positive to negative at age 48.91.
]


.pull-right[
```{r inflection, echo=FALSE}
age <- 18:65
marginal <- 0.3171-0.0356*(age-40)
ggplot(data.frame(age=age, marginal=marginal),
       aes(x=age, y=marginal))+
  geom_line()+
  geom_hline(yintercept = 0, linetype=2)+
  geom_vline(xintercept = 48.91, color="red")+
  labs(x="age",
       y="marginal effect of age on hourly wages",
       title="marginal effect graph")
```
]

---

## Plotting a polynomial fit

.pull-left[
```{r parabola-wages, fig.show="hide"}
ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width=1)+
  geom_smooth(se=FALSE, 
              method="lm", #<<
              formula=y~x+I(x^2))+ #<<
  labs(x="age", y="hourly wages")
```

in `geom_smooth`, you can specify a formula for the `lm` method that includes a squared term.
]

.pull-right[
```{r ref.label = 'parabola-wages', echo = FALSE}
```
]

---

## Higher order terms gives you more wiggle

.pull-left[
```{r poly-wiggle, fig.show="hide"}
ggplot(subset(gapminder, year==2007), 
       aes(x=gdpPercap, y=lifeExp))+
  geom_point()+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2),
              color="blue")+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2)+I(x^3),
              color="red")+
  geom_smooth(se=FALSE, 
              method="lm", 
              formula=y~x+I(x^2)+I(x^3)+I(x^4),
              color="green")+
    labs(x="GDP per capita", 
         y="life expectancy")
```

]

.pull-right[
```{r ref.label = 'poly-wiggle', echo = FALSE}
```
]

---

## Spline models

.pull-left[
```{r spline-wages, echo=FALSE}
earnings$age.spline <- ifelse(earnings$age<35, 0, earnings$age-35)
model <- lm(wages~age+age.spline, data=earnings)
predict_df <- data.frame(age=18:65)
predict_df$age.spline <- ifelse(predict_df$age<35, 0, predict_df$age-35)
predict_df$wages <- predict(model, newdata=predict_df)
ggplot(earnings, aes(x=age, y=wages))+
geom_jitter(alpha=0.01, width = 1)+
  geom_line(data=predict_df, size=1.5, color="blue")+
  geom_vline(xintercept = 35, color="red", linetype=2)+
  labs(y="hourly wages",
       title="spline model with hinge at age 35")
```
]

.pull-right[
- The basic idea of a spline model is to allow the slope of the relationship between $x$ and $y$ to be different at different cutpoints or "hinge" values of $x$. 

- These cutpoints create different linear segments where the effect of x on y is different. 

- We will look at the case of one hinge value which gives us an overall slope that looks like a "broken arrow."
]

---

## Creating the spline variable

The relationship between age and wages suggests that the relationship shifts considerably around age 35. To model this we create a spline variable like so:

$$spline_i=\begin{cases}
  age-35 & \text{if age>35}\\
  0 & \text{otherwise}
  \end{cases}$$

We can then add this variable to our model:
  
```{r spline} 
earnings$age.spline <- ifelse(earnings$age<35, 0, earnings$age-35)
model <- lm(wages~age+age.spline, data=earnings)
coef(model)
```

How do we interpret the results?

---

## Interpreting results in spline model

.pull-left[
Up to age 35, the spline term is zero, so our model is given by:

$$\hat{wages}_i=-6.04+0.9472(age_i)$$

The model predicts that for individuals age 35 and under, a one year increase in age is associated with a $0.9472 increase in wages, on average. 

After age 35, the spline variable increases by one every time age increases by one, so the marginal effect of age is given by:

$$0.9472-09549=-0.0077$$

The model predicts that for individuals over age 35, a one year increase in age is associated with a $0.0077 reduction in wages, on average.
]

.pull-right[
```{r spline-interpret, echo=FALSE}
ggplot(earnings, aes(x=age, y=wages))+
geom_jitter(alpha=0.005, width = 1)+
  geom_line(data=predict_df, size=1.5, color="blue")+
  geom_vline(xintercept = 35, color="red", linetype=2)+
  labs(y="hourly wages")+
  annotate("text", x=28, y=30, 
           label=paste("beta == ", 0.9472), 
           parse=TRUE,
           color="blue")+
  annotate("text", x=50, y=35, 
           label=paste("beta == ", -0.0077), 
           parse=TRUE,
           color="blue")
```
]

---

## {{< fa user-ninja >}} Plotting the spline model

.pull-left[
We cannot use `geom_smooth` to show the fit of this spline model. However, with some additional effort, we can plot the effect on a graph. 

1. Calculate the predicted value of wages for a range of age values based on the model using the `predict` command.
2. Feed in these predicted values as a dataset to the `geom_line` function in our `ggplot` graph.
]

--

.pull-right[
### The `predict` command

The `predict` command takes two important arguments:

1. The model object for which we want predicted values of the dependent variable.
2. A new dataset that contains all the same independent variables that are in the model.

```{r use-predict-spline}
model <- lm(wages~age+age.spline, data=earnings)
pre_df <- data.frame(age=18:65)
pre_df$age.spline <- ifelse(pre_df$age<35, 0,
                            pre_df$age-35)
pre_df$wages <- predict(model, 
                        newdata=pre_df)
head(pre_df, n=3)
```
]

---

## Adding the spline fit to ggplot

.pull-left[
```{r spline-wages-predict, fig.show="hide"}
ggplot(earnings, aes(x=age, y=wages))+
  geom_jitter(alpha=0.01, width = 1)+
  geom_line(data=predict_df, #<<
            color="blue", 
            size=1.5)
```

To add the spline fit, we just add a `geom_line` command and specify our `pre_df` dataset through the `data` argument so that it uses the predicted values rather than the actual `earnings` dataset to graph the line.
]

.pull-right[
```{r ref.label = 'spline-wages-predict', echo = FALSE}
```
]

